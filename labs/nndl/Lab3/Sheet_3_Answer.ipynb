{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MOitTdg3yP-M"
   },
   "source": [
    "<strong>                    Answer Sheet: Lab3 , Team : 15</strong>\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td>\n",
    "      <table>\n",
    "       <tr><td><b>Faiz Ahmed</b></td></tr>\n",
    "        <tr><td>1152231</td></tr>\n",
    "        <tr><td>stu225473@mail.uni-kiel.de</td></tr> \n",
    "      </table> \n",
    "    </td>\n",
    "    <td>\n",
    "      <table>\n",
    "        <tr><td><b>Mithun Das</b></td></tr>\n",
    "        <tr><td>1151651</td></tr>\n",
    "        <tr><td>stu225039@mail.uni-kiel.de</td></tr>\n",
    "      </table> \n",
    "    </td>\n",
    "    <td>\n",
    "      <table>\n",
    "        <tr><td><b>Mutasim Fuad Ansari</b></td></tr>\n",
    "        <tr><td>1152109</td></tr>\n",
    "        <tr><td>stu225365@mail.uni-kiel.de</td></tr>\n",
    "      </table> \n",
    "    </td>\n",
    "    <td>\n",
    "      <table>\n",
    "        <tr><td><b>Mohammad Abir Reza</b></td></tr>\n",
    "        <tr><td>1151705</td></tr>\n",
    "        <tr><td>stu225093@mail.uni-kiel.de</td></tr>\n",
    "      </table> \n",
    "    </td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ar1O5Y0j65JH"
   },
   "source": [
    "# Exercise 1 (Learning in neural networks)\n",
    "\n",
    "a) Explain the following terms related to neural networks as short and precise as possible. \n",
    "\n",
    "* Learning in neural networks\n",
    "* Training set\n",
    "* Supervised Learning\n",
    "* Unsupervised Learning\n",
    "* Online (incremental) learning\n",
    "* Offline (batch) learning\n",
    "* Training error\n",
    "* Generalisation error\n",
    "* Overfitting\n",
    "* Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "Y7gz0Xvt65JI"
   },
   "source": [
    "### Answer\n",
    "\n",
    "* Learning in neural networks :\n",
    "\"learning\" refers to specifying the organization of the network(connectivity, neuronal elementsetc.) in such a way that a desired network response is achieved for a given set of input patterns(the \"trainingset\")\n",
    "* Training set : Training set is Certain number of measured values, which is used as inputs and assesment of network output to fit the model. This is the actual dataset that we use to train the model .\n",
    "* Supervised Learning : Supervised learning is a machine learning technique using data corresponding to target output.That means data is already tagged with the correct answer.It can be compared to learning which takes place in the presence of a supervisor or a teacher.\n",
    "* Unsupervised Learning : Unsupervised learning data without targeted output. The network detects similarities and generates groups with similar characteristics. For example - Clustering problems.\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gp7zx4KQz3G7"
   },
   "source": [
    "* Online (incremental) learning : In online learning, machine learns after every training sample. \n",
    "* Offline (batch) learning : After inputing all training samples machine learns in offline learning.\n",
    "* Training error : Training error is the difference between network output and target output of training data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k3eNQwvUzmmM"
   },
   "source": [
    "* Generalisation error : Generalization error (also known as the out-of-sample error) is a measure of how accurately an algorithm is able to predict outcome values for previously unseen data. \n",
    "* Overfitting :Details of some training patterns are learned which are not relevant for most of the remaining patterns.It is caused for too much detail.\n",
    "\n",
    "* Cross-validation : The concept of Cross-validation is to split the dataset D(with P patterns) into several part. One part is taken as test set rest of the parts are used as trainig set. In the next step another part is taken as test case and other parts are used ase training set. If the number of partitions is k with 2 <= k <= p, the iterations goes for k times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d8ahoTHxBW82"
   },
   "source": [
    "b) Name and briefly describe at least two methods to indicate or avoid overfitting when training neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "Gc2H26Rb65JK"
   },
   "source": [
    "### Answer\n",
    "\n",
    "Two methods to avoid overfitting are described below :\n",
    "\n",
    "\n",
    "1.   Early Stopping : When validation error reach the minimum value, training need to be stopped. \n",
    "2.  Regulerization : Too many network parameters may lead to complexity of the network and also cause overfitting. There are different techniques for regularization. Dropout (reducing node in hidden layer), weight regularization by adding weight penalty(L1,l2 regularizaiton), Data Augmentation (a strategy that enables practitioners to significantly increase the diversity of data available for training models, without actually collecting new data) are useful strategies to implement regularization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dHDU9m2hChSn"
   },
   "source": [
    "# Exercise 2 (Perceptron learning – analytical calculation)\n",
    "\n",
    "The goal of this exercise is to train a single-layer perceptron (threshold element) to classify\n",
    "whether a fruit presented to the perceptron is going to be liked by a certain person or not,\n",
    "based on three features attributed to the presented fruit: its taste (whether it is sweet or not),\n",
    "its seeds (whether they are edible or not) and its skin (whether it is edible or not). This\n",
    "generates the following table for the inputs and the target output of the perceptron:\n",
    "\n",
    "Fruit | Input Taste<br>sweet = 1<br>not sweet = 0 | Input Seeds<br>edible = 1<br>not edible = 0 | Input Skin<br>edible = 1<br>not edible = 0 | Target output<br>person likes = 1<br>doesn’t like = 0\n",
    ":---|:---:|:---:|:---:|:---:\n",
    "Banana|1|1|0|1\n",
    "Pear|1|0|1|1\n",
    "Lemon|0|0|0|0\n",
    "Strawberry|1|1|1|1\n",
    "Green Apple|0|0|1|0\n",
    "\n",
    "Since there are three (binary) input values (taste, seeds and skin) and one (binary) target\n",
    "output, we will construct a single-layer perceptron with three inputs and one output.\n",
    "\n",
    "![IMAGE: perceptron](images/perceptron.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V2oFzm5K65JN"
   },
   "source": [
    "Since the target output is binary, we will use the perceptron learning algorithm to construct the weights.\n",
    "\n",
    "To start the perceptron learning algorithm, we have to initialize the weights and the threshold.\n",
    "Since we have no prior knowledge on the solution, we will assume that all weights are 0 ($w_1 = w_2 = w_3 = 0$) and that the threshold is $\\theta = 1$ (i.e. $w_0 = -\\theta = -1$).\n",
    "Furthermore, we have to specify the learning rate $\\eta$.\n",
    "Since we want it to be large enough that learning happens in a reasonable amount of time, but small enough so that it doesn’t go too fast, we set $\\eta = 0.25$.\n",
    "\n",
    "Apply the perceptron learning algorithm – in the incremental mode – analytically to this problem, i.e. calculate the new weights and threshold after successively presenting a banana, pear, lemon, strawberry and a green apple to the network (in this order).\n",
    "\n",
    "Draw a diagram of the final perceptron indicating the weight and threshold parameters and verify that the final perceptron classifies all training examples correctly.\n",
    "\n",
    "Note: The iteration of the perceptron learning algorithm is easily accomplished by filling in the following table for each iteration of the learning algorithm:\n",
    "\n",
    "First iteration ($\\mu = 1$), current training sample: banana\n",
    "\n",
    "Input<br> ${x}^{(\\mu)}$ | Current Weights<br>$w(t)$ | Network Output<br>$y^{(\\mu)}$ | Target Ouput<br>$d^{(\\mu)}$ | Learning rate<br>$\\eta$ | Weight Update<br>$\\Delta w(t)$ | New weights<br>$w(t+1)$\n",
    ":---:|:---:|:---:|:---:|:---:|:---:|:---:\n",
    "$x_0$ = 1 |  $w_0$ = | | | 0.25 | | \n",
    "$x_1$ =  |  $w_1$ = | | | 0.25 | | \n",
    "$x_2$ =  |  $w_2$ = | | | 0.25 | | \n",
    "$x_3$ =  |  $w_3$ = | | | 0.25 | | \n",
    "\n",
    "Second iteration ($\\mu = 2$), current training sample: pear\n",
    "...\n",
    "\n",
    "\n",
    "(Source of exercise: Langston, Cognitive Psychology)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "a4YKm74T65JP"
   },
   "source": [
    "### Answer\n",
    "\n",
    "**Iteration for Banana**\n",
    "\n",
    "From Given information Output for banana, d1 = 1     \n",
    "**Calculation :** \n",
    "\n",
    "\n",
    "\n",
    "Output y1  = Θ [1 * (-1) + 1 * 0 + 1 * 0 + 0 * 0] = Θ [-1] = 0\n",
    "\n",
    "w0(1) =  w0(0) +  $\\eta$ * (d1 - y1) * x0 = (-1) + 0.25 * (1 – 0) * 1 = -0.75\n",
    "\n",
    "w1 (1) = w1 (0) + $\\eta$ * (d1 - y1) * x1 = 0 + 0.25 * (1 – 0)  * 1 = 0.25\n",
    "\n",
    "w2 (1) = w2 (0) + $\\eta$ * (d1 - y1) * x2 = 0 + 0.25 * (1 – 0) * 1 = 0.25\n",
    "\n",
    "w3 (1) = w2 (0) + $\\eta$ * (d1 - y1) * x3 = 0 + 0.25 * (1 – 0) * 0 = 0\n",
    "\n",
    "$\\Delta w0$(1) = w0(1) - w0 (0) = -0.75 – (-1) = 0.25\n",
    "\n",
    "$\\Delta w1$(1) = w1(1) - w1 (0) = 0.25 – 0 = 0.25\n",
    "\n",
    "$\\Delta w2$(1) = w2(1) - w2 (0) = 0.25 – 0 = 0.25\n",
    "\n",
    "$\\Delta w3$(1) = w3(1) - w3 (0) = 0 – 0 = 0\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Input<br> ${x}^{(\\mu)}$ | Current Weights<br>$w(t)$ | Network Output<br>$y^{(\\mu)}$ | Target Ouput<br>$d^{(\\mu)}$ | Learning rate<br>$\\eta$ | Weight Update<br>$\\Delta w(t)$ | New weights<br>$w(t+1)$\n",
    ":---:|:---:|:---:|:---:|:---:|:---:|:---:\n",
    "$x_0$ = 1 |  $w_0$ = -1 | 0 | 1 | 0.25 | 0.25 | -.75  \n",
    "$x_1$ = 1 |  $w_1$ = 0  |   |   | 0.25 | 0.25 | 0.25\n",
    "$x_2$ = 1 |  $w_2$ = 0  |   |   | 0.25 | 0.25 | 0.25\n",
    "$x_3$ = 0 |  $w_3$ = 0  |   |   | 0.25 | 0.00 | 0\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**Iteration for Pear**\n",
    "\n",
    "From Given information Output for Pear, d2 = 1     \n",
    "**Calculation :** \n",
    "\n",
    "\n",
    "\n",
    "Output y2  = Θ [1 * (-0.75) + 1 * 0.25 + 0 * 0.25 + 1 * 0] = Θ [-0.50] = 0\n",
    "\n",
    "w0 (2) = w0 (1) + $\\eta$ * (d2 – y2) * x0 = (-0.75) + 0.25 * (1 – 0) * 1 = -0.50\n",
    "\n",
    "w1 (2) = w1 (1) + $\\eta$ * (d2 – y2) * x1 = 0.25 + 0.25 * (1 – 0)  * 1 = 0.50\n",
    "\n",
    "w2 (2) = w2 (1) + $\\eta$ * (d2 – y2) * x2 = 0.25 + 0.25 * (1 – 0) * 0 = 0.25\n",
    "\n",
    "w3 (2) = w2 (1) + $\\eta$ * (d2 – y2) * x3 = 0 + 0.25 * (1 – 0) * 1 = 0.25\n",
    "\n",
    "\n",
    "$\\Delta w0$(2) = w0(2) - w0 (1) = -0.50 – (-0.75) = 0.25\n",
    "\n",
    "$\\Delta w1$(2)= w1(2) - w1 (1) = 0.50 – 0.25 = 0.25\n",
    "\n",
    "$\\Delta w2$(2) = w2(2) - w2 (1) = 0.25 – 0.25 = 0\n",
    "\n",
    "$\\Delta w3$(2) = w3(2) - w3 (1) = 0.25 – 0 = 0.25\n",
    "\n",
    "\n",
    "\n",
    "Input<br> ${x}^{(\\mu)}$ | Current Weights<br>$w(t)$ | Network Output<br>$y^{(\\mu)}$ | Target Ouput<br>$d^{(\\mu)}$ | Learning rate<br>$\\eta$ | Weight Update<br>$\\Delta w(t)$ | New weights<br>$w(t+1)$\n",
    ":---:|:---:|:---:|:---:|:---:|:---:|:---:\n",
    "$x_0$ = 1 |  $w_0$ = -1 | 0 | 1 | 0.25 | 0.25 | -.50  \n",
    "$x_1$ = 1 |  $w_1$ = 0  |   |   | 0.25 | 0.25 | 0.50\n",
    "$x_2$ = 1 |  $w_2$ = 0  |   |   | 0.25 | 0    | 0.25\n",
    "$x_3$ = 0 |  $w_3$ = 0  |   |   | 0.25 | 0.00 | 0.25\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**Iteration for Lemon**\n",
    "\n",
    "From Given information Output for Lemon, d3 = 0    \n",
    "**Calculation :** \n",
    "\n",
    "\n",
    "\n",
    "Output y3  = Θ [1 * (-0.50) + 0 * 0.50 + 0 * 0.25 + 0 * 0.25] = Θ [-0.50] = 0\n",
    "\n",
    "w0 (3) = w0 (2) + $\\eta$ * (d3 – y3) * x0 = (-0.50) + 0.25 * (0 – 0) * 0 = -0.50\n",
    "\n",
    "w1 (3) = w1 (2) + $\\eta$ * (d3 – y3) * x1 =  0.50 + 0.25 * (0 – 0)  * 0 = 0.50\n",
    "\n",
    "w2 (3) = w2 (2) + $\\eta$ * (d3 – y3) * x2 = 0.25 + 0.25 * (0 – 0) * 0 = 0.25\n",
    "\n",
    "w3 (3) = w2 (2) + $\\eta$ * (d3 – y3) * x3 =  0.25 + 0.25 * (0 – 0) * 0 = 0.25\n",
    "\n",
    "\n",
    "$\\Delta w0$(3) = w0(3) - w0 (2) = -0.50 – (-0.50) = 0\n",
    "\n",
    "$\\Delta w1$(3)= w1(3) - w1 (2) = 0.50 – 0.50 = 0\n",
    "\n",
    "$\\Delta w2$(3) = w2(3) - w2 (2) = 0.25 – 0.25 = 0\n",
    "\n",
    "$\\Delta w3$(3) = w3(3) - w3 (2) = 0.25 – 0.25 = 0\n",
    "\n",
    "\n",
    "\n",
    "Input<br> ${x}^{(\\mu)}$ | Current Weights<br>$w(t)$ | Network Output<br>$y^{(\\mu)}$ | Target Ouput<br>$d^{(\\mu)}$ | Learning rate<br>$\\eta$ | Weight Update<br>$\\Delta w(t)$ | New weights<br>$w(t+1)$\n",
    ":---:|:---:|:---:|:---:|:---:|:---:|:---:\n",
    "$x_0$ = 1 |  $w_0$ = -1 | 0 | 0 | 0.25 | 0 | -.50  \n",
    "$x_1$ = 1 |  $w_1$ = 0  |   |   | 0.25 | 0 | 0.50\n",
    "$x_2$ = 1 |  $w_2$ = 0  |   |   | 0.25 | 0 | 0.25\n",
    "$x_3$ = 0 |  $w_3$ = 0  |   |   | 0.25 | 0.| 0.25\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**Iteration for Strawberry**\n",
    "\n",
    "From Given information Output for Strawberry, d4 = 1     \n",
    "**Calculation :** \n",
    "\n",
    "\n",
    "\n",
    "Output y4  = Θ [1 * (-0.50) + 1 * 0.50 + 1 * 0.25 + 1 * 0.25] = Θ [0.50] = 1\n",
    "\n",
    "w0 (4) = w0 (3) + $\\eta$ * (d4 – y4) * x0 = (-0.50) + 0.25 * (1 – 1) * 1 = -0.50\n",
    "\n",
    "w1 (4) = w1 (3) + $\\eta$ * (d4 – y4) * x1 = 0.50 + 0.25 * (1 – 1)  * 1 = 0.50\n",
    "\n",
    "w2 (4) = w2 (3) + $\\eta$ * (d4 – y4) * x2  = 0.25 + 0.25 * (1 – 1) * 1 = 0.25\n",
    "\n",
    "w3 (4) = w2 (3) + $\\eta$ * (d4 – y4) * x3 = 0.25 + 0.25 * (1 – 1) * 1 = 0.25\n",
    "\n",
    "\n",
    "$\\Delta w0$(4) = w0(4) - w0 (3) = -0.50 – (-0.50) = 0\n",
    "\n",
    "$\\Delta w1$(4)= w1(4) - w1 (3) = 0.50 – 0.50 = 0\n",
    "\n",
    "$\\Delta w2$(4) = w2(4) - w2 (3) = 0.25 – 0.25 = 0\n",
    "\n",
    "$\\Delta w3$(4) = w3(4) - w3 (3) = 0.25 – 0.25 = 0\n",
    "\n",
    "\n",
    "\n",
    "Input<br> ${x}^{(\\mu)}$ | Current Weights<br>$w(t)$ | Network Output<br>$y^{(\\mu)}$ | Target Ouput<br>$d^{(\\mu)}$ | Learning rate<br>$\\eta$ | Weight Update<br>$\\Delta w(t)$ | New weights<br>$w(t+1)$\n",
    ":---:|:---:|:---:|:---:|:---:|:---:|:---:\n",
    "$x_0$ = 1 |  $w_0$ = -1 | 1 | 1 | 0.25 | 0 | -.50  \n",
    "$x_1$ = 1 |  $w_1$ = 0  |   |   | 0.25 | 0 | 0.50\n",
    "$x_2$ = 1 |  $w_2$ = 0  |   |   | 0.25 | 0 | 0.25\n",
    "$x_3$ = 0 |  $w_3$ = 0  |   |   | 0.25 | 0.| 0.25\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**Iteration for Green Apple**\n",
    "\n",
    "From Given information Output for Green Apple, d5 = 0\n",
    "**Calculation :** \n",
    "\n",
    "\n",
    "\n",
    "Output y5  = Θ [1 * (-0.50) + 0 * 0.50 + 0 * 0.25 + 1 * 0.25] = Θ [-0.25] = 0\n",
    "\n",
    "w0 (5) = w0 (4) + $\\eta$ * (d5 – y5) * x0 = (-0.50) + (-0.50) + 0.25 * (0 – 0) * 1 = -0.50\n",
    "\n",
    "w1 (5) = w1 (4) + $\\eta$ * (d5 – y5) * x1 = 0.50 + 0.25 * (0 – 0)  * 0 = 0.50\n",
    "\n",
    "w2 (5) = w2 (4) + $\\eta$ * (d5 – y5) * x2  = 0.25 + 0.25 * (0 – 0) * 0 = 0.25\n",
    "\n",
    "w3 (5) = w2 (4) + $\\eta$ * (d5 – y5) * x3 = 0.25 + 0.25 * (0 – 0) * 1 = 0.25\n",
    "\n",
    "\n",
    "$\\Delta w0$(5) = w0(5) - w0 (4) = -0.50 – (-0.50) = 0\n",
    "\n",
    "$\\Delta w1$(5)= w1(5) - w1 (4) = 0.50 – 0.50 = 0\n",
    "\n",
    "$\\Delta w2$(5) = w2(5) - w2 (4) = 0.25 – 0.25 = 0\n",
    "\n",
    "$\\Delta w3$(5) = w3(5) - w3 (4) = 0.25 – 0.25 = 0\n",
    "\n",
    "\n",
    "\n",
    "Input<br> ${x}^{(\\mu)}$ | Current Weights<br>$w(t)$ | Network Output<br>$y^{(\\mu)}$ | Target Ouput<br>$d^{(\\mu)}$ | Learning rate<br>$\\eta$ | Weight Update<br>$\\Delta w(t)$ | New weights<br>$w(t+1)$\n",
    ":---:|:---:|:---:|:---:|:---:|:---:|:---:\n",
    "$x_0$ = 1 |  $w_0$ = -1 | 0 | 0 | 0.25 | 0 | -.50  \n",
    "$x_1$ = 1 |  $w_1$ = 0  |   |   | 0.25 | 0 | 0.50\n",
    "$x_2$ = 1 |  $w_2$ = 0  |   |   | 0.25 | 0 | 0.25\n",
    "$x_3$ = 0 |  $w_3$ = 0  |   |   | 0.25 | 0.| 0.25\n",
    "\n",
    "\n",
    "Here after all 5 training set we get w1 = 0.50 , w2 = 0.25 , w3 = 0.25 , $\\theta$ = 0.50.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V0dZkCh_Cm2n"
   },
   "source": [
    "# Exercise 3 (Single-layer perceptron, gradient learning, 2dim. classification)\n",
    "\n",
    "The goal of this exercise is to solve a two-dimensional binary classification problem with gradient learning, using TensorFlow.\n",
    "Since the problem is two-dimensional, the perceptron has 2 inputs. Since the classification problem is binary, there is one output.\n",
    "\n",
    "The (two-dimensional) inputs for training are provided in the file *exercise3b_input.txt*, the corresponding (1-dimensional) targets in the file *exercise3b_target.txt*. To visualize the results, the training samples corresponding to class 1 (output label “0”) have separately been saved in the file *exercise3b_class1.txt*, the training samples corresponding to class 2 (output label “1”) in the file *exercise3b_class2.txt*.\n",
    "\n",
    "The gradient learning algorithm – using the sigmoid activation function – shall be used to provide a solution to this classification problem. Note that due to the sigmoid activation function, the output of the perceptron is a real value in [0,1]:\n",
    "\n",
    "\\begin{equation}\n",
    "sigmoid(h) = \\frac{1}{1+e^{-h}}\n",
    "\\end{equation}\n",
    "To assign a binary class label (either 0 or 1) to an input example, the perceptron output $y$ can\n",
    "be passed through the Heaviside function $\\theta [ y - 0.5 ] $ to yield a binary output $y^{binary}$.\n",
    "Then, any perceptron output between 0.5 and 1 is closer to 1 than to 0 and will be assigned the class label “1”.\n",
    "Conversely, any perceptron output between 0 and $<0.5$ is closer to 0 than to 1 and will be assigned the class label “0”.\n",
    "As usual, denote the weights of the perceptron $w_1$ and $w_2$ and the bias $w_0 = -\\theta $.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QUX94DzGW0Jh"
   },
   "source": [
    "## Task a)\n",
    "\n",
    "Using the above-mentioned post-processing step $\\theta [ y - 0.5 ] $  applied to the perceptron output $y$, show that the decision boundary separating the inputs $x=( x_1 , x_2 )$ assigned to class label “1” from those inputs assigned to class label “0” is given by a straight line in two-dimensional space corresponding to the equation (see in python code at *# plot last decision boundary*): \n",
    "\n",
    "$x_2 = -\\frac{w_1}{w_2}x_1 - \\frac{w_0}{w_2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "zFhvlGiy65JT"
   },
   "source": [
    "### Answer\n",
    "\n",
    "Write your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1bOBn82oXXU7"
   },
   "source": [
    "## Task b)\n",
    "\n",
    "The classification problem (defined by the training data provided in *exercise3b_input.txt* and the targets provided in *exercise3b_target.txt*)\n",
    "shall now be solved using the TensorFlow and Keras libraries.\n",
    "The source code is given below and can be executed by clicking the play button (in colab or in a local installation with tensorflow and keras).\n",
    "\n",
    "1.   Train the model at least three times and report on your findings.\n",
    "2.   Change appropriate parameters (e.g. the learning rate, the batch size, the choice of the solver, potentially the number of epochs etc.) and again report on your findings.\n",
    "\n",
    "Useful information about training and evaluation with Tensorflow and Keras can be found at\n",
    "https://www.tensorflow.org/guide/keras/train_and_evaluate\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B6tA_K3-BAyw"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from os.path import join\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import Model, Input\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "\n",
    "###-----------------\n",
    "# load training data\n",
    "###-----------------\n",
    "path_to_task = \"nndl/Lab3\"\n",
    "input = np.loadtxt(join(path_to_task,'exercise3b_input.txt'))\n",
    "tmp = np.loadtxt(join(path_to_task,'exercise3b_target.txt'))\n",
    "target = np.array([tmp[i] for i in range(tmp.size)])\n",
    "class1 = np.loadtxt(join(path_to_task,'exercise3b_class1.txt'))\n",
    "class2 = np.loadtxt(join(path_to_task,'exercise3b_class2.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iF1QcuBwBFdW"
   },
   "source": [
    "\n",
    "Define the neural network, here you can change the structure of network, the learning rate and the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HprUKTffLDnq"
   },
   "outputs": [],
   "source": [
    "# Define the structure\n",
    "input_layer = Input(shape=(2,), name='input') # two dimensional input\n",
    "out = Dense(units=1, activation=\"sigmoid\", name=\"output\")(input_layer) # one ouput node with sigmoid activation\n",
    "\n",
    "# create a model\n",
    "model = Model(input_layer, out)\n",
    "\n",
    "# show how the model looks\n",
    "model.summary()\n",
    "\n",
    "# compile the model\n",
    "opt = SGD(learning_rate=0.1)\n",
    "model.compile(optimizer=opt,loss=\"binary_crossentropy\",metrics=[\"acc\"])\n",
    "\n",
    "# try to invoke one of the weight initializers\n",
    "# initializer = tf.keras.initializers.GlorotUniform()\n",
    "# shape = (2,1) # n_in, n_out\n",
    "# random_weights = tf.Variable(initializer(shape=shape)) # returns tensor object; how to convert to numpy array needed for set_weights?\n",
    "# random_weights = np.random.uniform(low = -1.0, high = 1.0, size=(2,1))\n",
    "\n",
    "# weights: list; index 0: weights (numpy array of shape n_in x n_out), index 1: biases (numpy array)\n",
    "# model.set_weights([ random_weights, np.array([0])])\n",
    "\n",
    "# save initial weights\n",
    "initial_weights = model.layers[-1].get_weights() \n",
    "\n",
    "print(\"initial weights: (%f, %f)\" % (initial_weights[0][0], initial_weights[0][1]))\n",
    "print(\"initial bias: %f\" % initial_weights[1][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "idMWZMOMYMv5"
   },
   "source": [
    "This line actually trains the model. Changeable parameters batch_size and epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q0YZ0ZFcOdI1"
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(x=input, y=target, batch_size=1, epochs=100, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sNL1qLxHYVbO"
   },
   "source": [
    "The following code snippet plots the results you create in the snippet before.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Dycq1s9UJ20r"
   },
   "outputs": [],
   "source": [
    "# plot setup\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 15))\n",
    "legend = []\n",
    "\n",
    "# plot the data\n",
    "axes[0].set_title('Toy classification problem: Data and decision boundaries')\n",
    "axes[0].set_xlabel('x1')\n",
    "axes[0].set_ylabel('x2')\n",
    "\n",
    "minx = min(input[:,0])\n",
    "maxx = max(input[:,0])\n",
    "miny = min(input[:,1])\n",
    "maxy = max(input[:,1])\n",
    "axes[0].set_xlim(minx, maxx)\n",
    "axes[0].set_ylim(miny, maxy) \n",
    "axes[0].plot(class1[:,0], class1[:,1], 'r.', \\\n",
    "    class2[:,0], class2[:,1], 'b.')\n",
    "legend.append('samples class1')\n",
    "legend.append('samples class2')\n",
    "\n",
    "# initial weights \n",
    "w0 = initial_weights[1][0] # bias\n",
    "# weight components (list of of numpy arrays of shape n_in x n_out)\n",
    "w1 = initial_weights[0][0][0]\n",
    "w2 = initial_weights[0][1][0]\n",
    "if ( w2 == 0 ):\n",
    "    print(\"Error: second weight zero!\")\n",
    "\n",
    "# calculate initial decision boundary\n",
    "interval = np.arange( np.floor(minx), np.ceil(maxx), 0.1 )\n",
    "initial_decision_boundary = -w1*interval/w2 - w0/w2\n",
    "\n",
    "# plot initial decision boundary\n",
    "args = {'c': 'black', 'linestyle': 'dashed'}\n",
    "axes[0].plot( interval, initial_decision_boundary, **args)\n",
    "legend.append('initial decision boundary')\n",
    "\n",
    "# get final weights\n",
    "final_weights = model.layers[-1].get_weights()\n",
    "w0 = final_weights[1][0] # bias\n",
    "# weight components (list of of numpy arrays of shape n_in x n_out)\n",
    "w1 = final_weights[0][0][0]\n",
    "w2 = final_weights[0][1][0]\n",
    "if ( w2 == 0 ):\n",
    "    print(\"Error: second weight zero!\")\n",
    "\n",
    "print(\"final weights: (%f, %f)\" % (final_weights[0][0], final_weights[0][1]))\n",
    "print(\"final bias: %f\" % final_weights[1][0])\n",
    "\n",
    "# calculate final decision boundary\n",
    "interval = np.arange( np.floor(minx), np.ceil(maxx), 0.1 )\n",
    "final_decision_boundary = -w1*interval/w2 - w0/w2\n",
    "\n",
    "# plot final decision boundary\n",
    "args = {'c': 'black', 'linestyle': '-'}\n",
    "axes[0].plot( interval, final_decision_boundary, **args)\n",
    "legend.append('final decision boundary')\n",
    "\n",
    "# plot training loss  \n",
    "axes[1].plot(history.history['loss'])\n",
    "axes[1].set_title('Toy classification problem: Loss curve')\n",
    "axes[1].set_xlabel('Epoch number')\n",
    "axes[1].set_ylim(0, 1)\n",
    "axes[1].set_ylabel('loss')\n",
    "\n",
    "# plot training accuracy  \n",
    "axes[2].plot(history.history['acc'])\n",
    "axes[2].set_title('Toy classification problem: acc curve')\n",
    "axes[2].set_ylim(0, 1)\n",
    "axes[2].set_xlabel('Epoch number')\n",
    "axes[2].set_ylabel('acc')\n",
    "\n",
    "# show the plot\n",
    "fig.legend(axes[0].get_lines(), legend, ncol=3, loc=\"upper center\")\n",
    "plt.show()\n",
    "\n",
    "# final evaluation (here: on the training data)\n",
    "eval = model.evaluate(x=input, y=target)\n",
    "print(\"Final loss: %f, final accuray: %f\" % (eval[0], eval[1]))\n",
    "\n",
    "predictions = model.predict(x=input)\n",
    "binary_predictions = np.heaviside(predictions - 0.5, 1) # second argument: output in case the input is 0\n",
    "binary_predictions = binary_predictions.reshape(target.shape)\n",
    "\n",
    "abs_binary_errors = np.where(binary_predictions != target)[0].size # np.where returns tuple, [0] recovers numpy array\n",
    "rel_binary_errors = abs_binary_errors / len(target)\n",
    "print(\"\\nnumber of binary errors: %d, error rate: %f, accuracy: %f\" % (abs_binary_errors, rel_binary_errors, 1.0 - rel_binary_errors ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "vvPCYMYo65Jw"
   },
   "source": [
    "### Answer\n",
    "\n",
    "1)\n",
    "\n",
    "##### **Output: 1st Tranining**\n",
    "final weights: (-32.864176, 9.796065)\n",
    "\n",
    "final bias: 6.534341\n",
    "\n",
    "Final loss: 0.355930, final accuray: 0.920000\n",
    "\n",
    "number of binary errors: 12, error rate: 0.055600, accuracy: 0.920000\n",
    "\n",
    "\n",
    "##### **Output: 2nd Tranining**\n",
    "final weights: (-25.286375, 7.396742)\n",
    "\n",
    "final bias: 5.090887\n",
    "\n",
    "Final loss: 0.215550, final accuray: 0.945000\n",
    "\n",
    "number of binary errors: 11, error rate: 0.058000, accuracy: 0.945000\n",
    "\n",
    "##### **Output: 3rd Tranining**\n",
    "final weights: (-25.686739, 8.692582)\n",
    "\n",
    "final bias: 5.945707\n",
    "\n",
    "Final loss: 0.269067, final accuray: 0.920000\n",
    "\n",
    "number of binary errors: 10, error rate: 0.066000, accuracy: 0.\n",
    "\n",
    "\n",
    "2)\n",
    "\n",
    "#### **configuration: solver=Adam, epoch=25, lr=0.1**\n",
    "\n",
    "final weights: (-31.854929, 11.147592)\n",
    "\n",
    "final bias: 5.103813\n",
    "\n",
    "Final loss: 0.282556, final accuray: 0.930000\n",
    "\n",
    "number of binary errors: 11, error rate: 0.058000, accuracy: 0.930000\n",
    "\n",
    "####** configuration: solver=Adam, epoch=45, lr=0.5**\n",
    "\n",
    "final weights: (-78.309929, 24.959489)\n",
    "\n",
    "final bias: 15.855894\n",
    "\n",
    "Final loss: 0.245165, final accuray: 0.905000\n",
    "\n",
    "number of binary errors: 21, error rate: 0.105000, accuracy: 0.905000\n",
    "\n",
    "#### **configuration: solver=Adam, epoch=100, lr=0.01**\n",
    "\n",
    "final weights: (-22.957848, 9.416083)\n",
    "\n",
    "final bias: 3.290746\n",
    "\n",
    "Final loss: 0.263088, final accuray: 0.935000\n",
    "\n",
    "number of binary errors: 12, error rate: 0.070000, accuracy: 0.935000\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bCk4EokGYqSi"
   },
   "source": [
    "## Task c)\n",
    "\n",
    "Repeat exercise b) with the training set *exercise3c_input.txt* and the targets *exercise3c_target.txt*. Those points have been generated from the input points of exercise b) by removing points from class 1 (i.e. those points the x-coordinate of which is below 0.35). Do not forget to modify the variables *class1* and *class2* to load the files *exercise3c_class1.txt* and *exercise3c_class1.txt*, respectively! Discuss the output of the training algorithm in terms of the resulting decision boundary and the final training error.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "oDqpKUZ965Jy"
   },
   "source": [
    "### Answer\n",
    "\n",
    "#### **configuration: solver=Adam, epoch=30, lr=0.1**\n",
    "\n",
    "final weights: (-76.197903, 13.683178)\n",
    "\n",
    "final bias: 19.684738\n",
    "\n",
    "Final loss: 0.085936,\n",
    "\n",
    "final accuray: 0.958293\n",
    "\n",
    "number of binary errors: 5,\n",
    "\n",
    "error rate: 0.021939,\n",
    "\n",
    "accuracy: 0.958293\n",
    "\n",
    "#### **configuration: solver=Adam, epoch=35, lr=0.5**\n",
    "\n",
    "final weights: (-95.036801, 16.9800853)\n",
    "\n",
    "final bias: 28.787293\n",
    "\n",
    "Final loss: 0.066991, \n",
    "\n",
    "final accuray: 0.959958\n",
    "\n",
    "number of binary errors: 7,\n",
    "\n",
    "error rate: 0.0390048, \n",
    "\n",
    "accuracy: 0.959958\n",
    "\n",
    "#### **configuration: solver=Adam, epoch=35, lr=0.01**\n",
    "\n",
    "final weights: (-11.901829, 3.766939)\n",
    "\n",
    "final bias: 3.761029\n",
    "\n",
    "Final loss: 0.428174, \n",
    "\n",
    "final accuray: 0.971513\n",
    "\n",
    "number of binary errors: 2,\n",
    "\n",
    "error rate: 0.020870, \n",
    "\n",
    "accuracy: 0.971513"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "75PmmHWC65J0"
   },
   "source": [
    "## Task d)\n",
    "\n",
    "Divide the input samples from part b) into separate training and validation sets, where the latter shall comprise 30% of the data. You may use available Keras functionality for this purpose. Run the script at least two times, plot the training and validation loss and accuracy as a function of the epoch number and report on your findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "aoAli1pj65J2"
   },
   "source": [
    "### Answer\n",
    "\n",
    "### **First Run**\n",
    "\n",
    "final weights: (-34.234521, 11.390290)\n",
    "\n",
    "final bias: 5.839820\n",
    "\n",
    "Final loss: 0.170456,\n",
    "\n",
    "final accuray: 0.937000\n",
    "\n",
    "\n",
    "number of binary errors: 11, \n",
    "\n",
    "error rate: 0.056000, \n",
    "\n",
    "accuracy: 0.937000\n",
    "\n",
    "![IMAGE: d1](images/d1.png)\n",
    "\n",
    "\n",
    "### **Second Run**\n",
    "\n",
    "final weights: (-37.384506, 13.697980)\n",
    "\n",
    "final bias: 7.137096\n",
    "\n",
    "Final loss: 0.161900, \n",
    "\n",
    "final accuray: 0.9350000\n",
    "\n",
    "number of binary errors: 11,\n",
    "\n",
    "error rate: 0.065000, \n",
    "\n",
    "accuracy: 0.935000\n",
    "\n",
    "![IMAGE: d2](images/d2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P7nhiCqM65J5"
   },
   "source": [
    "## Task e)\n",
    "\n",
    "Modify the script to handle the XOR-problem, i.e. set\n",
    "\n",
    "input = np.array([[0,0],[0,1],[1,0],[1,1]])<br>\n",
    "target = np.array([0, 1, 1, 0])\n",
    "\n",
    "and plot the final decision boundary and the loss function. Report on your findings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "2PHPa-YJ65J6"
   },
   "source": [
    "### Answer\n",
    "\n",
    "Write your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "6CNO3hsl65J7"
   },
   "source": [
    "# Exercise 4 (Multi-layer perceptron and backpropagation – small datasets)\n",
    "\n",
    "The goal of this exercise is to apply a multi-layer perceptron (MLP), trained with the backpropagation algorithm as provided by Tensorflow Keras library, to four classification problems provided by the UCI repository (and contained in the scikit learn package; i.e. iris, digits, wine, breast_cancer) and two artificially generated classification problems (circles, moon). In particular, the influence of the backpropagation solver and of the network topology shall be investigated in parts a) and b) of the exercise, respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tgwQvZNn65J7"
   },
   "source": [
    "## Task a)\n",
    "\n",
    "In this part of the exercise, a number of solvers (stochastic gradient descent, Adam, Adam with Nesterov momentum, AdaDelta, AdaGrad or RMSProp) shall be applied to the six datasets. An (incomplete) python script for this experiment is provided the Jupyter notebook. Complete the code (model definition, selection and configuration of an optimizer and model “compilation” including selection of an appropriate loss function; see *# TO BE ADAPTED* in the Jupyter notebook); consult the Tensorflow Keras documentation if needed. Furthermore, select suitable values of the most important parameters (e.g. learning rate, batch size...). Then, apply the script for at least three different optimizers, for a suitable baseline model configuration. Report the final training and validation loss and accuracy values and provide plots for the training and validation loss and accuracy curves as a function of the number of epochs (see script). What are your conclusions regarding the comparison of the optimization strategies? Also report on the database statistics.\n",
    "\n",
    "The optimizer is selected e.g. with <br>\n",
    "*opt = SGD(learning_rate=lr) # SGD or Adam, Nadam, Adadelta, Adagrad, RMSProp* <br>\n",
    "Note that additional parameters of the optimizers can be set if desired (see the Tensorflow Keras documentation).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ws9AUFb-65J8"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from os.path import join\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import Model, Input\n",
    "from tensorflow.keras.optimizers import SGD, Adam, Adadelta, Adagrad, Nadam, RMSprop\n",
    "from tensorflow.keras.utils import normalize\n",
    "from sklearn import datasets\n",
    "\n",
    "###--------\n",
    "# load data\n",
    "###--------\n",
    "\n",
    "data_sets = ['iris', 'digits', 'wine', 'breast_cancer', 'circles', 'moons']\n",
    "histories = {}\n",
    "final_training_loss = {}\n",
    "final_training_accuracy = {}\n",
    "final_validation_loss = {}\n",
    "final_validation_accuracy = {}\n",
    "\n",
    "for name in data_sets:\n",
    "  print(\"\\nProcessing data set %s\" % name)\n",
    "  if name == 'iris':\n",
    "    iris = datasets.load_iris()\n",
    "    input = iris.data \n",
    "    target = iris.target \n",
    "  elif name == 'digits':    \n",
    "    digits = datasets.load_digits()\n",
    "    input = digits.data\n",
    "    target = digits.target\n",
    "  elif name == 'wine':\n",
    "    wine = datasets.load_wine() \n",
    "    input = wine.data\n",
    "    target = wine.target\n",
    "  elif name == 'breast_cancer':\n",
    "    breast_cancer = datasets.load_breast_cancer() \n",
    "    input = breast_cancer.data\n",
    "    target = breast_cancer.target\n",
    "  elif name == 'circles':\n",
    "    circles = datasets.make_circles(noise=0.2, factor=0.5, random_state=1)\n",
    "    input = circles[0]\n",
    "    target = circles[1]\n",
    "  elif name == 'moons':\n",
    "    moons = datasets.make_moons(noise=0.3, random_state=0)\n",
    "    input = moons[0]\n",
    "    target = moons[1]\n",
    "  else:\n",
    "    print(\"name %s unknown\" % name)\n",
    "  input_dim = input.shape[1]\n",
    "  print(\"input dimension: %d\" % input_dim)  \n",
    "  print(\"input shape: \" + str(input.shape))\n",
    "  print(\"target shape: \" + str(target.shape))\n",
    "  num_classes = len(np.unique(target))\n",
    "  print(\"number of classes: %d\" % num_classes)\n",
    "  print(\"class labels: \" + str(np.unique(target)))\n",
    "\n",
    "  ###-----------\n",
    "  # process data\n",
    "  ###-----------\n",
    "\n",
    "  # shuffle data\n",
    "  data = np.column_stack((input, target))\n",
    "  np.random.shuffle(data)\n",
    "  input = data[:,np.arange(input_dim)] # columns 0 ... input_dim - 1 (contain input features)\n",
    "  target = data[:,input_dim] # column input_dim (contains targets)\n",
    "\n",
    "  # normalize inputs\n",
    "  mean = np.mean(input)\n",
    "  std = np.std(input, ddof=1)\n",
    "  input = (input - mean) / std\n",
    "\n",
    "  # if necessary, transform labels to be in range 0 ... num_classes - 1\n",
    "  labels_for_one_hot = {}\n",
    "  for i in range(num_classes):\n",
    "    labels_for_one_hot[np.unique(target)[i]] = i\n",
    "\n",
    "  # one-hot encoding\n",
    "  def one_hot(j):\n",
    "    vec = np.zeros(num_classes)\n",
    "    vec[j] = 1\n",
    "    return vec\n",
    "\n",
    "  # transform targets to one-hot encoding\n",
    "  target_one_hot = np.zeros((len(target), num_classes))\n",
    "  for i in range(len(target)):\n",
    "    target_one_hot[i] = one_hot( labels_for_one_hot[int(target[i])] )\n",
    "\n",
    "  ###-----------\n",
    "  # define model\n",
    "  ###-----------\n",
    "\n",
    "  # Define the structure of the neural network\n",
    "  num_inputs = input_dim\n",
    "  num_hidden = 100 # TO BE ADAPTED\n",
    "  num_outputs = num_classes\n",
    "  input_layer = Input(shape=(num_inputs,), name='input') # two dimensional input\n",
    "  \n",
    "  hidden_1 = Dense(units=num_hidden, activation=\"relu\", name=\"hidden_layer\")(input_layer) # TO BE ADAPTED\n",
    "  hidden_2 = Dense(units=num_hidden, activation=\"relu\", name=\"hidden_layer2\")(hidden_1) # TO BE ADAPTED\n",
    "  out = Dense(units=num_classes, activation=(\"sigmoid\" if num_classes==2 else \"softmax\"), name=\"final_output\")(hidden_2) # TO BE ADAPTED \n",
    "  \n",
    "  # create a model\n",
    "  model = Model(input_layer, out)\n",
    "\n",
    "  # show how the model looks\n",
    "  model.summary()\n",
    "\n",
    "  # compile the model\n",
    "  lr = 0.01 # TO BE ADAPTED\n",
    "  opt = SGD(learning_rate=lr) # TO BE ADAPTED # SGD or Adam, Nadam, Adadelta, Adagrad, RMSProp\n",
    "  loss_function = \"binary_crossentropy\" if num_classes==2 else \"categorical_crossentropy\"\n",
    "  model.compile(optimizer=opt,loss=loss_function ,metrics=[\"categorical_accuracy\"]) # TO BE ADAPTED\n",
    "\n",
    "  ###-------\n",
    "  # training\n",
    "  ###-------\n",
    "\n",
    "  # Train the model\n",
    "  num_epochs = 100 # TO BE ADAPTED\n",
    "  batch_size = 1 # TO BE ADAPTED\n",
    "  history = model.fit(x=input, y=target_one_hot, batch_size=batch_size, epochs=num_epochs, verbose=True, validation_split = 0.3)\n",
    "  histories[name] = history\n",
    "  final_training_loss[name] = history.history['loss'][num_epochs-1]\n",
    "  final_training_accuracy[name] = history.history['categorical_accuracy'][num_epochs-1]\n",
    "  final_validation_loss[name] = history.history['val_loss'][num_epochs-1]\n",
    "  final_validation_accuracy[name] = history.history['val_categorical_accuracy'][num_epochs-1]\n",
    "\n",
    "for name in data_sets: \n",
    "  print(\"\\n%s:\\n\" % name)\n",
    "  print(\"final training loss: %f\" % final_training_loss[name])\n",
    "  print(\"final training accuracy: %f\" % final_training_accuracy[name])\n",
    "  print(\"final validation loss: %f\" % final_validation_loss[name])\n",
    "  print(\"final validation accuracy: %f\" % final_validation_accuracy[name])\n",
    "\n",
    "###-----------\n",
    "# plot results\n",
    "###-----------\n",
    " \n",
    "# plot setup\n",
    "fig, axes = plt.subplots(3, 2, figsize=(15, 10))\n",
    "fig.tight_layout() # improve spacing between subplots, doesn't work\n",
    "plt.subplots_adjust(left=0.125, right=0.9, bottom=0.1, top=0.9, wspace=0.2, hspace=0.2) # doesn't work\n",
    "legend = []\n",
    "i = 0\n",
    "axes_indices = {0 : (0,0), 1 : (0,1), 2: (1,0), 3: (1,1), 4: (2,0), 5: (2,1)}\n",
    "\n",
    "for name in data_sets:\n",
    "  # plot loss    \n",
    "  axes[axes_indices[i]].set_title(name)\n",
    "  if i == 4 or i == 5:  \n",
    "    axes[axes_indices[i]].set_xlabel('Epoch number')\n",
    "  axes[axes_indices[i]].set_ylim(0, 1)\n",
    "  axes[axes_indices[i]].plot(histories[name].history['loss'], color = 'blue', \n",
    "              label = 'training loss')\n",
    "  axes[axes_indices[i]].plot(histories[name].history['val_loss'], color = 'red', \n",
    "              label = 'validation loss')\n",
    "  axes[axes_indices[i]].legend()\n",
    "\n",
    "  # plot accuracy  \n",
    "  axes[axes_indices[i]].plot(histories[name].history['categorical_accuracy'], color = 'blue', linestyle = 'dashed',\n",
    "              label = 'training accuracy')\n",
    "  axes[axes_indices[i]].plot(histories[name].history['val_categorical_accuracy'], color = 'red', linestyle = 'dashed', \n",
    "              label = 'validation accuracy')\n",
    "  axes[axes_indices[i]].legend()\n",
    "  i = i + 1\n",
    "\n",
    "\n",
    "# show the plot\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "z8QRLfFC65KB"
   },
   "source": [
    "### Answer\n",
    "#### Iteration 1 , Optimizer : SGD, Learning Rate : 0.01####\n",
    "\n",
    "**iris:**\n",
    "\n",
    "final training loss: 0.072402\n",
    "\n",
    "final training accuracy: 0.980952\n",
    "\n",
    "final validation loss: 0.050727\n",
    "\n",
    "final validation accuracy: 0.977778\n",
    "\n",
    "\n",
    "**digits:**\n",
    "\n",
    "\n",
    "final training loss: 0.000850\n",
    "\n",
    "final training accuracy: 1.000000\n",
    "\n",
    "final validation loss: 0.104347\n",
    "\n",
    "final validation accuracy: 0.977778\n",
    "\n",
    "\n",
    "**wine:**\n",
    "\n",
    "final training loss: 0.566051\n",
    "\n",
    "final training accuracy: 0.709677\n",
    "\n",
    "final validation loss: 0.573185\n",
    "\n",
    "final validation accuracy: 0.666667\n",
    "\n",
    "\n",
    "**breast_cancer:**\n",
    "\n",
    "final training loss: 0.152266\n",
    "\n",
    "final training accuracy: 0.949749\n",
    "\n",
    "final validation loss: 0.249697\n",
    "\n",
    "final validation accuracy: 0.883041\n",
    "\n",
    "\n",
    "**circles:**\n",
    "\n",
    "final training loss: 0.332946\n",
    "\n",
    "final training accuracy: 0.885714\n",
    "\n",
    "final validation loss: 0.327478\n",
    "\n",
    "final validation accuracy: 0.900000\n",
    "\n",
    "\n",
    "**moons:**\n",
    "\n",
    "final training loss: 0.263171\n",
    "\n",
    "final training accuracy: 0.885714\n",
    "\n",
    "final validation loss: 0.318069\n",
    "\n",
    "final validation accuracy: 0.900000\n",
    "\n",
    "\n",
    "![IMAGE: SGD](images/4_SGD.png)\n",
    "\n",
    "\n",
    "#### Iteration 2 , Optimization Method: Adadelta, Training rate: 0.01 ####\n",
    "\n",
    "**iris:**\n",
    "\n",
    "final training loss: 0.579217\n",
    "\n",
    "final training accuracy: 0.790476\n",
    "\n",
    "final validation loss: 0.605583\n",
    "\n",
    "final validation accuracy: 0.800000\n",
    "\n",
    "\n",
    "**digits:**\n",
    "\n",
    "final training loss: 0.190644\n",
    "\n",
    "final training accuracy: 0.951472\n",
    "\n",
    "final validation loss: 0.211463\n",
    "\n",
    "final validation accuracy: 0.940741\n",
    "\n",
    "\n",
    "**wine:**\n",
    "\n",
    "final training loss: 0.839041\n",
    "\n",
    "final training accuracy: 0.604839\n",
    "\n",
    "final validation loss: 0.894024\n",
    "\n",
    "final validation accuracy: 0.666667\n",
    "\n",
    "\n",
    "**breast_cancer:**\n",
    "\n",
    "final training loss: 0.234960\n",
    "\n",
    "final training accuracy: 0.919598\n",
    "\n",
    "final validation loss: 0.290192\n",
    "\n",
    "final validation accuracy: 0.894737\n",
    "\n",
    "\n",
    "**circles:**\n",
    "\n",
    "final training loss: 0.663483\n",
    "\n",
    "final training accuracy: 0.542857\n",
    "\n",
    "final validation loss: 0.681810\n",
    "\n",
    "final validation accuracy: 0.466667\n",
    "\n",
    "\n",
    "**moons:**\n",
    "\n",
    "final training loss: 0.513655\n",
    "\n",
    "final training accuracy: 0.800000\n",
    "\n",
    "final validation loss: 0.580649\n",
    "\n",
    "final validation accuracy: 0.766667\n",
    "\n",
    "![IMAGE: Adadelta](images/Adadelta.png)\n",
    "\n",
    "\n",
    "#### Iteration 3 , Opt : RMSProp, Iteration rate : 0.01\n",
    "\n",
    "**iris:**\n",
    "\n",
    "final training loss: 0.063413\n",
    "\n",
    "final training accuracy: 0.980952\n",
    "\n",
    "final validation loss: 0.590287\n",
    "\n",
    "final validation accuracy: 0.911111\n",
    "\n",
    "**digits:**\n",
    "\n",
    "final training loss: 0.000000\n",
    "\n",
    "final training accuracy: 1.000000\n",
    "\n",
    "final validation loss: 0.522021\n",
    "\n",
    "final validation accuracy: 0.981481\n",
    "\n",
    "**wine:**\n",
    "\n",
    "final training loss: 0.752013\n",
    "\n",
    "final training accuracy: 0.733871\n",
    "\n",
    "final validation loss: 0.749712\n",
    "\n",
    "final validation accuracy: 0.777778\n",
    "\n",
    "**breast_cancer:**\n",
    "\n",
    "final training loss: 0.212206\n",
    "\n",
    "final training accuracy: 0.934673\n",
    "\n",
    "final validation loss: 0.149893\n",
    "\n",
    "final validation accuracy: 0.959064\n",
    "\n",
    "\n",
    "**circles:**\n",
    "\n",
    "final training loss: 0.176986\n",
    "\n",
    "final training accuracy: 0.942857\n",
    "\n",
    "final validation loss: 0.723873\n",
    "\n",
    "final validation accuracy: 0.900000\n",
    "\n",
    "\n",
    "**moons:**\n",
    "\n",
    "final training loss: 0.188970\n",
    "\n",
    "final training accuracy: 0.985714\n",
    "\n",
    "final validation loss: 0.178707\n",
    "\n",
    "final validation accuracy: 0.966667\n",
    "\n",
    "![IMAGE: RMSprop](images/RMSprop.png)\n",
    "\n",
    "\n",
    "\n",
    "#### Conclusion ####\n",
    "According to the graphs, we can say that Optimization strategy **SGD** is best among three of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ig1yViuM65KD"
   },
   "source": [
    "## Task b)\n",
    "\n",
    "Using the most successful optimizer from part a), in this part of the exercise different network topologies shall be investigated, i.e. the number of hidden layers and of hidden neurons shall be varied. To this end, modify the python script accordingly and systematically test the network performance. Provide the final training and validation loss and accuracy and provide the loss and accuracy curves as function of the number of epochs. You may also test further parameter settings. What are your conclusions regarding the network topology?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "jz0RcX0P65KG"
   },
   "source": [
    "### Answer\n",
    "\n",
    "#### Opt: SGD, Hidden Layer : 1, Hidden Neurons : 50 ####\n",
    "\n",
    "**iris:**\n",
    "\n",
    "final training loss: 0.047442\n",
    "\n",
    "final training accuracy: 0.990476\n",
    "\n",
    "final validation loss: 0.170617\n",
    "\n",
    "final validation accuracy: 0.911111\n",
    "\n",
    "\n",
    "**digits:**\n",
    "\n",
    "final training loss: 0.001037\n",
    "\n",
    "final training accuracy: 1.000000\n",
    "\n",
    "final validation loss: 0.071326\n",
    "\n",
    "final validation accuracy: 0.981481\n",
    "\n",
    "\n",
    "**wine:**\n",
    "\n",
    "final training loss: 0.546695\n",
    "\n",
    "final training accuracy: 0.733871\n",
    "\n",
    "final validation loss: 0.585359\n",
    "\n",
    "final validation accuracy: 0.611111\n",
    "\n",
    "\n",
    "**breast_cancer:**\n",
    "\n",
    "final training loss: 0.154872\n",
    "\n",
    "final training accuracy: 0.939699\n",
    "\n",
    "final validation loss: 0.208050\n",
    "\n",
    "final validation accuracy: 0.918129\n",
    "\n",
    "\n",
    "**circles:**\n",
    "\n",
    "final training loss: 0.281645\n",
    "\n",
    "final training accuracy: 0.914286\n",
    "\n",
    "final validation loss: 0.336740\n",
    "\n",
    "final validation accuracy: 0.833333\n",
    "\n",
    "\n",
    "**moons:**\n",
    "\n",
    "final training loss: 0.318536\n",
    "\n",
    "final training accuracy: 0.857143\n",
    "\n",
    "final validation loss: 0.221821\n",
    "\n",
    "final validation accuracy: 0.900000\n",
    "\n",
    "![IMAGE: SGD_L1_N50](images/SGD_L1_N50.png)\n",
    "\n",
    "\n",
    "#### Opt: SGD, Hidden Layer : 1, Hidden Neurons : 100 ####\n",
    "\n",
    "**iris:**\n",
    "\n",
    "final training loss: 0.072402\n",
    "\n",
    "final training accuracy: 0.980952\n",
    "\n",
    "final validation loss: 0.050727\n",
    "\n",
    "final validation accuracy: 0.977778\n",
    "\n",
    "\n",
    "**digits:**\n",
    "\n",
    "\n",
    "final training loss: 0.000850\n",
    "\n",
    "final training accuracy: 1.000000\n",
    "\n",
    "final validation loss: 0.104347\n",
    "\n",
    "final validation accuracy: 0.977778\n",
    "\n",
    "\n",
    "**wine:**\n",
    "\n",
    "final training loss: 0.566051\n",
    "\n",
    "final training accuracy: 0.709677\n",
    "\n",
    "final validation loss: 0.573185\n",
    "\n",
    "final validation accuracy: 0.666667\n",
    "\n",
    "\n",
    "**breast_cancer:**\n",
    "\n",
    "final training loss: 0.152266\n",
    "\n",
    "final training accuracy: 0.949749\n",
    "\n",
    "final validation loss: 0.249697\n",
    "\n",
    "final validation accuracy: 0.883041\n",
    "\n",
    "\n",
    "**circles:**\n",
    "\n",
    "final training loss: 0.332946\n",
    "\n",
    "final training accuracy: 0.885714\n",
    "\n",
    "final validation loss: 0.327478\n",
    "\n",
    "final validation accuracy: 0.900000\n",
    "\n",
    "\n",
    "**moons:**\n",
    "\n",
    "final training loss: 0.263171\n",
    "\n",
    "final training accuracy: 0.885714\n",
    "\n",
    "final validation loss: 0.318069\n",
    "\n",
    "final validation accuracy: 0.900000\n",
    "\n",
    "\n",
    "![IMAGE: SGD_L1_N100](images/4_SGD.png)\n",
    "\n",
    "#### Opt: SGD, Hidden Layer : 1, Hidden Neurons : 150 ####\n",
    "\n",
    "**iris:**\n",
    "\n",
    "final training loss: 0.069464\n",
    "\n",
    "final training accuracy: 0.980952\n",
    "\n",
    "final validation loss: 0.057866\n",
    "\n",
    "final validation accuracy: 1.000000\n",
    "\n",
    "\n",
    "**digits:\n",
    "\n",
    "final training loss: 0.000863\n",
    "\n",
    "final training accuracy: 1.000000\n",
    "\n",
    "final validation loss: 0.041053\n",
    "\n",
    "final validation accuracy: 0.981481\n",
    "\n",
    "\n",
    "**wine:\n",
    "\n",
    "final training loss: 0.528201\n",
    "\n",
    "final training accuracy: 0.741935\n",
    "\n",
    "final validation loss: 0.639217\n",
    "\n",
    "final validation accuracy: 0.648148\n",
    "\n",
    "\n",
    "**breast_cancer:\n",
    "\n",
    "final training loss: 0.181956\n",
    "\n",
    "final training accuracy: 0.917085\n",
    "\n",
    "final validation loss: 0.154664\n",
    "\n",
    "final validation accuracy: 0.929825\n",
    "\n",
    "\n",
    "**circles:\n",
    "\n",
    "final training loss: 0.283462\n",
    "\n",
    "final training accuracy: 0.928571\n",
    "\n",
    "final validation loss: 0.374384\n",
    "\n",
    "final validation accuracy: 0.866667\n",
    "\n",
    "\n",
    "**moons:\n",
    "\n",
    "final training loss: 0.224648\n",
    "\n",
    "final training accuracy: 0.942857\n",
    "\n",
    "final validation loss: 0.539297\n",
    "\n",
    "final validation accuracy: 0.700000\n",
    "\n",
    "![IMAGE: SGD_L1_N150](images/SGD_L1_N150.png)\n",
    "\n",
    "\n",
    "### Opt : SGD, Hidden Layer : 2 , Neurons: 150 \n",
    "\n",
    "Processing data set iris\n",
    "input dimension: 4\n",
    "input shape: (150, 4)\n",
    "target shape: (150,)\n",
    "number of classes: 3\n",
    "class labels: [0 1 2]\n",
    "Model: \"model_12\"\n",
    "_________________________________________________________________\n",
    "##### Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "input (InputLayer)           [(None, 4)]               0         \n",
    "_________________________________________________________________\n",
    "hidden_layer (Dense)         (None, 150)               750       \n",
    "_________________________________________________________________\n",
    "hidden_layer2 (Dense)        (None, 150)               22650     \n",
    "_________________________________________________________________\n",
    "##### final_output (Dense)         (None, 3)                 453       \n",
    "=================================================================\n",
    "Total params: 23,853\n",
    "\n",
    "\n",
    "==========================================================================\n",
    "\n",
    "**iris:**\n",
    "\n",
    "final training loss: 0.078948\n",
    "\n",
    "final training accuracy: 0.961905\n",
    "\n",
    "final validation loss: 0.044705\n",
    "\n",
    "final validation accuracy: 1.000000\n",
    "\n",
    "\n",
    "**digits:**\n",
    "\n",
    "final training loss: 0.000216\n",
    "\n",
    "final training accuracy: 1.000000\n",
    "\n",
    "final validation loss: 0.109893\n",
    "\n",
    "final validation accuracy: 0.977778\n",
    "\n",
    "\n",
    "**wine:**\n",
    "\n",
    "final training loss: 0.582043\n",
    "\n",
    "final training accuracy: 0.669355\n",
    "\n",
    "final validation loss: 0.527156\n",
    "\n",
    "final validation accuracy: 0.814815\n",
    "\n",
    "\n",
    "**breast_cancer:**\n",
    "\n",
    "final training loss: 0.170408\n",
    "\n",
    "final training accuracy: 0.924623\n",
    "\n",
    "final validation loss: 0.184235\n",
    "\n",
    "final validation accuracy: 0.929825\n",
    "\n",
    "\n",
    "**circles:**\n",
    "\n",
    "final training loss: 0.230768\n",
    "\n",
    "final training accuracy: 0.885714\n",
    "\n",
    "final validation loss: 0.155399\n",
    "\n",
    "final validation accuracy: 0.933333\n",
    "\n",
    "\n",
    "**moons:**\n",
    "\n",
    "final training loss: 0.147394\n",
    "\n",
    "final training accuracy: 0.942857\n",
    "\n",
    "final validation loss: 0.213510\n",
    "\n",
    "final validation accuracy: 0.966667\n",
    "\n",
    "![IMAGE:SGD_L2_N150](images/SGD_L2_N150.png)\n",
    "\n",
    "\n",
    "#### Opt: SGD , Hidden layer : 2, Neurons: 100\n",
    "\n",
    "**iris:**\n",
    "\n",
    "final training loss: 0.043750\n",
    "\n",
    "final training accuracy: 0.971429\n",
    "\n",
    "final validation loss: 0.019908\n",
    "\n",
    "final validation accuracy: 1.000000\n",
    "\n",
    "**digits:**\n",
    "\n",
    "final training loss: 0.000223\n",
    "\n",
    "final training accuracy: 1.000000\n",
    "\n",
    "final validation loss: 0.108348\n",
    "\n",
    "final validation accuracy: 0.979630\n",
    "\n",
    "**wine:**\n",
    "\n",
    "final training loss: 0.523984\n",
    "\n",
    "final training accuracy: 0.709677\n",
    "\n",
    "final validation loss: 0.622331\n",
    "\n",
    "final validation accuracy: 0.740741\n",
    "\n",
    "**breast_cancer:**\n",
    "\n",
    "final training loss: 0.172765\n",
    "\n",
    "final training accuracy: 0.932161\n",
    "\n",
    "final validation loss: 0.172633\n",
    "\n",
    "final validation accuracy: 0.929825\n",
    "\n",
    "**circles:**\n",
    "\n",
    "final training loss: 0.163506\n",
    "\n",
    "final training accuracy: 0.914286\n",
    "\n",
    "final validation loss: 0.316757\n",
    "\n",
    "final validation accuracy: 0.933333\n",
    "\n",
    "**moons:**\n",
    "\n",
    "final training loss: 0.094640\n",
    "\n",
    "final training accuracy: 0.971429\n",
    "\n",
    "final validation loss: 0.241356\n",
    "\n",
    "final validation accuracy: 0.933333\n",
    "\n",
    "![IMAGE: SGD_L2_N100](images/SGD_L2_N100.png)\n",
    "\n",
    "\n",
    "#### Accuracy Table (HL = # hidden layer, N = # of Neurons ) ####\n",
    "\n",
    "| Sample | HL 1, N 50 | HL 1, N 100 | HL 1, N 150 | HL 2, N 100 | HL 1, N 150 |\n",
    "| --- | --- | --- |---| --- | ---|\n",
    "| iris |0.990476| 0.980952 | 0.980952 | .971429 | 0.961905 |\n",
    "| digits |1.000000| 1.000000 | 1.000000 | 1.000000 | 1.000000 |\n",
    "| wine |0.733871| 0.709677 | 0.741935 | 0.709677 | 0.669355 |\n",
    "| breast_cancer |0.939699| 0.949749 | 0.917085 | 0.932161 | 0.924623 |\n",
    "| circles |0.914286| 0.885714 | 0.928571 | 0.914286 |  0.885714 |\n",
    "| moons |0.857143| 0.885714 | 0.942857 | 0.971429 | 0.942857 |\n",
    "\n",
    "\n",
    "#### Conclusion ####\n",
    "According to the previoius table, We can say that(w.r.t Accuracy) , Network with Hidden Layer 1 and Number of Neuron 50 is the best network.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Sheet_3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
