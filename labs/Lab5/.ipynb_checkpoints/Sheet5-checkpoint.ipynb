{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uqPLT3QAGdqK"
   },
   "source": [
    "<b> Team 15, Lab : 5 </b>\n",
    "<table>\n",
    "  <tr>\n",
    "    <td>\n",
    "      <table>\n",
    "       <tr><td><b>Faiz Ahmed</b></td></tr>\n",
    "        <tr><td>1152231</td></tr>\n",
    "        <tr><td>stu225473@mail.uni-kiel.de</td></tr> \n",
    "      </table> \n",
    "    </td>\n",
    "    <td>\n",
    "      <table>\n",
    "        <tr><td><b>Mithun Das</b></td></tr>\n",
    "        <tr><td>1151651</td></tr>\n",
    "        <tr><td>stu225039@mail.uni-kiel.de</td></tr>\n",
    "      </table> \n",
    "    </td>\n",
    "    <td>\n",
    "      <table>\n",
    "        <tr><td><b>Mutasim Fuad Ansari</b></td></tr>\n",
    "        <tr><td>1152109</td></tr>\n",
    "        <tr><td>stu225365@mail.uni-kiel.de</td></tr>\n",
    "      </table> \n",
    "    </td>\n",
    "    <td>\n",
    "      <table>\n",
    "        <tr><td><b>Mohammad Abir Reza</b></td></tr>\n",
    "        <tr><td>1151705</td></tr>\n",
    "        <tr><td>stu225093@mail.uni-kiel.de</td></tr>\n",
    "      </table> \n",
    "    </td>\n",
    "  </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "WVqw2SetGdqY"
   },
   "source": [
    "## Exercise 1 (Loss functions and weight update formulae, theoretical considerations):\n",
    "\n",
    "a)\tShow that for a single-layer perceptron with a linear activation function, the weights can be determined in closed-form from the training data, assuming a mean-squared error loss.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tp4KN0kaGdqa"
   },
   "source": [
    "### Answer\n",
    "\n",
    "Please find the answer below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kUgoDXL2Gdqb"
   },
   "source": [
    "b) The cross-entropy loss for a single-layer  perceptron on a training set $ D= \\{( x^{(1)},y^{(1)}), ( x^{(2)},y^{(2)}), ... , ( x^{(p)},y^{(p)}) \\} $ is defined as \n",
    "$$ L_{CE} (w,y,\\hat{y}) = - \\frac{1}{p} \\sum_{\\mu = 1} ^p [ y^{(\\mu)} \\ln \\hat{y} (w,x^{(\\mu)}) + (1-y^{(\\mu)}) \\ln ( 1 - \\hat{y} (w, x^{(\\mu)}))]$$      \n",
    "where $\\hat{y} = \\hat{y}(w,x^{(\\mu)})$ is the output of the perceptron. Show that this expression is non-negative for $y,\\hat{y} \\in [0,1]$ and assumes a minimum (as a function of the perceptron output $\\hat{y}$) if $\\hat{y}(w,x^{(\\mu)} ) =  y^{(\\mu)}$ for all $\\mu$, i.e. if the neuron output $\\hat{y}(w,x^{(\\mu)})$  corresponds to the target value $y^{(μ)}$ for each training sample $\\mu$. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D3P8SgsrGdqc"
   },
   "source": [
    "### Answer\n",
    "\n",
    "Please find the answer below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gGUWClifGdqd"
   },
   "source": [
    "## Exercise 2 (Convolutional neural networks):\n",
    "\n",
    "a) Explain the following terms related to learning in neural networks:\n",
    "\n",
    "- Convolutional neural network\n",
    "- Filter, Kernel\n",
    "- Feature map\n",
    "- Receptive field\n",
    "- Pooling (subsampling) layer\n",
    "- Fully convolutional network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JAyesGf9Gdqe"
   },
   "source": [
    "### Answer\n",
    "\n",
    "**Convolutional neural network:**\n",
    "\n",
    "\n",
    "A convolutional neural network is a process of deep neural network.It is often use to analyze visual image. It can be said that it is use as CNN or ConvNet. multilayer perceptrons has different versions. CNNs are called to be the regularized versions of multilayer perceptrons. This perceptrons usually occurs in one layer and all the other layer get conneted with it via network. And thus with the help of “fully-connectedness” process of these networks make them overfitting data. However, a different way towards regularization usually taken by CNNs: because of the hierarchical patten in data and assemble more complex patters they use this method to get benifited by comparing this patterns. This is one of the big reason that CNNs are more ahead than other in lower extreme to be exact.\n",
    "\n",
    "**Filter, Kernel:**\n",
    "\n",
    "A Filter or kernel known as component of CNN but not defined. But this is a part of layered architecture from CNN.When it comes to a practical example in Convolutional neural network a kernel is a smaller-sized matrix in comparison to the input dimensions of the image. ‘activation maps’ is called the input volume to obtain. The activated regions shows where features get to reconige the input. When the training set execute the learning process change the kernel matrix and show the result that indicate the correct region or the nearest region.\n",
    "\n",
    "\n",
    "**Feature Map:** \n",
    "\n",
    "A feature map is a complete output of a particular kernel to an proper input. With the given parameter such as input size of the image or the type of filter used in the image ot the padding giving around the input shows the whole area of the feature map. In the output image, it shows variety of places with feature map in the input image. Every filter in the input images redirect to its completely uniqe feature maps in the output image.Getting several features in the input image is possible after several filter in the same input image.\n",
    "\n",
    "**Receptive field:** \n",
    "\n",
    "A receptive field is basically a connectivity of the local region of the input volume that a connected with each neuron that get as an input of huge or big size images.At the time of using a huge size images as input that has high dimensions in it,it is not possible to connect each neuron with all neurons in the network.\n",
    "\n",
    "**Pooling:** \n",
    "\n",
    "A Pooling is a layer that adds the activations of large number of sections in a single section of the current layer. Pooling layer occurs in every feature map unconventionaly. Maximum activation of every units in the receptive field in Convolutional neural network that commonly used are max pooling that uses maximum activation of all units.\n",
    "\n",
    "**Fully convolutional network:** \n",
    "\n",
    "A fully convolutional network is a kind of network that connects without any fully connected layers hence all teh layers in the network are convolution layer. Difference between other network with fully convolutional network is that it fully connected layers is just as end to end learable as a fully convolutional network.It tries to make a proper decision on the local input it got in the networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2lYczpUTGdqs"
   },
   "source": [
    "b) (CNN on MNIST) The Jupyter notebook provides code to apply a CNN to the MNIST classification problem. Complete the script – choosing a suitable network architecture and providing values for the hyperparamters – and report the accuracy of the model. Try to improve the accuracy of the CNN (and / or to reduce the number of model parameters) by modifying the model structure and / or the values of the hyperparameters and settings. Visualize and discuss the model structure. Compare your results to the results of MNIST classification using a multilayer perceptron (lab sheet 4, exercise 3). <br> \n",
    "\n",
    "Note: The model can be visualized using the *plot_model* method of *tensorflow.keras.utils*: <br>\n",
    "*plot_model(model, to_file='model.png', show_shapes=False, show_layer_names=True)* <br>\n",
    "This should write a visualization of the model to the file *model.png*. However, this may not work in Colab. In this case, you may execute the method locally (without training). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fy2G02RuGdqt"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten, Activation, BatchNormalization, GaussianNoise\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.optimizers import SGD, Adam, Adadelta, Adagrad, Nadam, RMSprop, schedules\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow.keras.datasets as tfds\n",
    "import tensorflow.keras.initializers as tfi\n",
    "import tensorflow.keras.regularizers as tfr\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import backend as K\n",
    "import os\n",
    "import datetime\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import sys\n",
    "\n",
    "###--------\n",
    "# load data\n",
    "###--------\n",
    "\n",
    "(training_input, training_target), (test_input, test_target)  = tfds.mnist.load_data()\n",
    "training_input = training_input.astype('float32')\n",
    "test_input = test_input.astype('float32')\n",
    "\n",
    "# plot some sample images\n",
    "num_examples = 3\n",
    "for s in range(num_examples):\n",
    "  print(\"Example image, true label: %d\" % training_target[s])\n",
    "  plt.imshow(training_input[s], vmin=0, vmax=255, cmap=plt.cm.gray)\n",
    "  plt.show()\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "num_classes = 10 # MNIST: 10 digits    # FIX!!!\n",
    "training_target = to_categorical(training_target, num_classes)\n",
    "test_target = to_categorical(test_target, num_classes)\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    training_input = training_input.reshape(training_input.shape[0], 1, img_rows, img_cols)\n",
    "    test_input = test_input.reshape(test_input.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    training_input = training_input.reshape(training_input.shape[0], img_rows, img_cols, 1)\n",
    "    test_input = test_input.reshape(test_input.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "# Reserve 10,000 samples for validation\n",
    "validation_input = training_input[-10000:]\n",
    "validation_target = training_target[-10000:]\n",
    "training_input = training_input[:-10000]\n",
    "training_target = training_target[:-10000]\n",
    "\n",
    "print(\"training input shape: %s, training target shape: %s\"  % (training_input.shape, training_target.shape))\n",
    "print(\"validation input shape: %s, validation target shape: %s\"  % (validation_input.shape, validation_target.shape))\n",
    "print(\"test input shape: %s, test target shape: %s\"  % (test_input.shape, test_target.shape))\n",
    "# range of input values: 0 ... 255\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "###-----------\n",
    "# process data\n",
    "###-----------\n",
    "\n",
    "# Note: shuffling is performed in fit method\n",
    "\n",
    "# scaling inputs from range 0 ... 255 to range [0,1] if desired\n",
    "scale_inputs = True # scale inputs to range [0,1]\n",
    "if scale_inputs:\n",
    "  training_input = training_input / 255\n",
    "  validation_input = validation_input / 255 \n",
    "  test_input = test_input / 255\n",
    "\n",
    "print(\"min. training data: %f\" % np.min(training_input))\n",
    "print(\"max. training data: %f\" % np.max(training_input))\n",
    "print(\"min. validation data: %f\" % np.min(validation_input))\n",
    "print(\"max. validation data: %f\" % np.max(validation_input))\n",
    "print(\"min. test data: %f\" % np.min(test_input))\n",
    "print(\"max. test data: %f\" % np.max(test_input))\n",
    "\n",
    "# histograms of input values\n",
    "nBins = 100\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15,10))\n",
    "axes[0].hist(training_input.flatten(), nBins)\n",
    "axes[0].set_xlabel(\"training\")\n",
    "axes[0].set_ylabel(\"counts\")\n",
    "axes[0].set_ylim((0,1e6))\n",
    "\n",
    "axes[1].hist(validation_input.flatten(), nBins)\n",
    "axes[1].set_xlabel(\"validation\")\n",
    "axes[1].set_ylabel(\"counts\")\n",
    "axes[1].set_ylim((0,1e6))\n",
    "axes[1].set_title('historgrams of input values')\n",
    "\n",
    "axes[2].hist(test_input.flatten(), nBins)\n",
    "axes[2].set_xlabel(\"test\")\n",
    "axes[2].set_ylabel(\"counts\")\n",
    "axes[2].set_ylim((0,1e6))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "###-----------\n",
    "# define model\n",
    "###-----------\n",
    "\n",
    "learningRate = .2 # FIX!!!\n",
    "learningRateSchedule = False # FIX!!! # True: apply (exponential) learning rate schedule; False: constant learning rate\n",
    "if learningRateSchedule == True:\n",
    "  lr_schedule = schedules.ExponentialDecay(initial_learning_rate = learningRate, decay_steps=100000, decay_rate=0.96, staircase=True) # or PiecewiseConstantDecay or PolynomialDecay or InverseTimeDecay \n",
    "  print(\"... applying exponential decay learning rate schedule with initial learning rate %f\" % learningRate)\n",
    "else:\n",
    "  lr_schedule = learningRate # constant learning rate\n",
    "  print(\"... constant learning rate %f\" % learningRate)\n",
    "\n",
    "dropout = .19 # FIX!!! # 0 if no dropout, else fraction of dropout units (e.g. 0.2)   # FIX!!!\n",
    "batch_normalization = True # FIX!!!\n",
    "data_augmentation = False # FIX !!!\n",
    "weight_init = tfi.glorot_uniform() # default: glorot_uniform(); e.g. glorot_normal(), he_normal(), he_uniform(), lecun_normal(), lecun_uniform(), RandomNormal(), RandomUniform(), Zeros() etc.\n",
    "bias_init = tfi.Zeros() # default: Zeros(); for some possible values see weight initializers\n",
    "regularization_weight = .0001 # FIX!!! # 0 for no regularization or e.g. 0.0001 to apply regularization # FIX!!!\n",
    "regularizer = tfr.l2(l=regularization_weight) # or l2 or l1_l2; used for both weights and biases # FIX!!!\n",
    "momentum = .98 # FIX!!! # or e.g. 0.9, 0.99; ONLY FOR STOCHASTIC GRADIENT DESCENT AND RMSPROP\n",
    "nesterov = True # FIX1!! # ONLY FOR STOCHASTIC GRADIENT DESCENT\n",
    "stddev = 0 # FIX!!! # 0 for not adding Gaussian noise; otherwise, initial stddev for adding Gaussian noise in convolutional layers (e.g. 0.2) # FIX!!!\n",
    "\n",
    "num_inputs = training_input.shape[1] \n",
    "num_outputs = num_classes \n",
    "\n",
    "activation = 'relu' # FIX!!!\n",
    "\n",
    "solver =  'RMSprop' # FIX!!!\n",
    "\n",
    "num_epochs = 10  # FIX !!!   \n",
    "batch_size = 100 # FIX !!!  \n",
    "\n",
    "# Sequential network structure.\n",
    "num_feature_maps_per_layer = [2] # FIX!!!\n",
    "num_hidden_neurons_fully_connected = [180] # FIX!!! \n",
    "kernel_size = 2 # FIX!!!\n",
    "pool_size = 3 # FIX!!!\n",
    "padding = 'same' \n",
    "\n",
    "experiment_name = \"\"\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "if len(num_feature_maps_per_layer) == 0:\n",
    "  print(\"Error: Must at least have one convolutional layer!\")\n",
    "  sys.exit()  \n",
    "\n",
    "# add first convolutional layer connecting to input layer\n",
    "num_feature_maps = num_feature_maps_per_layer[0]\n",
    "model.add(Conv2D(filters=num_feature_maps, kernel_size=kernel_size, input_shape=input_shape, padding=padding))\n",
    "if batch_normalization:\n",
    "  model.add(BatchNormalization())\n",
    "model.add(Activation(activation)) \n",
    "model.add(Conv2D(filters=num_feature_maps, kernel_size=kernel_size, padding=padding))\n",
    "if batch_normalization:\n",
    "  model.add(BatchNormalization())\n",
    "model.add(Activation(activation))  \n",
    "model.add(MaxPooling2D(pool_size=pool_size))\n",
    "#model.add(Dropout(0.25)) # normally not used\n",
    "if stddev > 0:\n",
    "  print(\"First convolutional layer: Adding additive Gaussian noise with stddev %f\" % stddev)\n",
    "  model.add(GaussianNoise(stddev=stddev)) \n",
    "\n",
    "# potentially further hidden layers\n",
    "for i in range(1, len(num_feature_maps_per_layer)):\n",
    "  # add convolutional layers with corresponding number of feature maps\n",
    "  num_feature_maps = num_feature_maps_per_layer[i] \n",
    "  model.add(Conv2D(filters=num_feature_maps, kernel_size=kernel_size, padding=padding))\n",
    "  if batch_normalization:\n",
    "    model.add(BatchNormalization())\n",
    "  model.add(Activation(activation))   \n",
    "  model.add(Conv2D(filters=num_feature_maps, kernel_size=kernel_size, padding=padding))\n",
    "  if batch_normalization:\n",
    "    model.add(BatchNormalization())\n",
    "  model.add(Activation(activation))   \n",
    "  model.add(MaxPooling2D(pool_size=pool_size))\n",
    "  if stddev > 0:\n",
    "    stddev = stddev + 0.1 # increase value of stddev with each convolutional block\n",
    "    print(\"Convolutional layer %d: Adding additive Gaussian noise with stddev %f\" % (i, stddev))\n",
    "    model.add(GaussianNoise(stddev=stddev)) \n",
    "#  model.add(Dropout(0.2 + i/10.0)) # normally not used\n",
    "\n",
    "# fully connected layers\n",
    "model.add(Flatten())\n",
    "for i in range(len(num_hidden_neurons_fully_connected)):\n",
    "  # add fully connected layers with corresponding number of hidden neurons\n",
    "  num_hidden = num_hidden_neurons_fully_connected[i]\n",
    "  model.add(Dense(num_hidden, activation=activation))\n",
    "  if dropout:\n",
    "    model.add(Dropout(dropout))\n",
    "  if batch_normalization:\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "# output layer\n",
    "model.add(Dense(units=num_outputs, name = \"output\", activation = 'softmax', kernel_initializer=weight_init, bias_initializer = bias_init, kernel_regularizer=regularizer, bias_regularizer=regularizer))\n",
    "  \n",
    "# print configuration\n",
    "print(\"\\nModel configuration: \")\n",
    "print(model.get_config())\n",
    "print(\"\\n\")\n",
    "print(\"... number of layers: %d\" % len(model.layers))\n",
    "\n",
    "# show how the model looks\n",
    "model.summary()\n",
    "      \n",
    "# compile model\n",
    "if solver == 'SGD':        \n",
    "  opt = SGD(learning_rate=lr_schedule, momentum=momentum, nesterov=nesterov) # SGD or Adam, Nadam, Adadelta, Adagrad, RMSProp, potentially setting more parameters\n",
    "elif solver == 'Adam':\n",
    "  opt = Adam(learning_rate=lr_schedule) \n",
    "elif solver == 'Nadam':\n",
    "  opt = Nadam(learning_rate=lr_schedule) # Nadam doesn't support adaptive learning rate schedule!\n",
    "elif solver == 'Adadelta':\n",
    "  opt = Adadelta(learning_rate=lr_schedule) \n",
    "elif solver == 'Adagrad':\n",
    "  opt = Adagrad(learning_rate=lr_schedule) \n",
    "elif solver == 'RMSprop':\n",
    "  opt = RMSprop(learning_rate=lr_schedule, momentum = momentum)\n",
    "model.compile(optimizer=opt,loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),metrics=['categorical_accuracy'])\n",
    "\n",
    "\n",
    "###----------\n",
    "# train model\n",
    "###----------\n",
    "\n",
    "# prepare tensorboard\n",
    "logs_base_dir = \"./logs\"\n",
    "os.makedirs(logs_base_dir, exist_ok=True)\n",
    "logdir = os.path.join(logs_base_dir, experiment_name, datetime.datetime.now().strftime(\"%d-%m-%H:%M:%S\"))\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "lr_callback = tf.keras.callbacks.LearningRateScheduler(lr_schedule)\n",
    "\n",
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    history = model.fit(training_input, training_target, epochs=num_epochs, \n",
    "                        batch_size=batch_size, shuffle=\"True\", verbose=2, \n",
    "                        validation_data=(test_input, test_target), callbacks=[tensorboard_callback])\n",
    "\n",
    "else:\n",
    "    #data augmentation\n",
    "    print('Using data augmentation.')\n",
    "    datagen = ImageDataGenerator(\n",
    "       rotation_range=15, \n",
    "       width_shift_range=0.1,\n",
    "       height_shift_range=0.1,\n",
    "       horizontal_flip=True,\n",
    "       )\n",
    "    datagen.fit(training_input)\n",
    "    history = model.fit(datagen.flow(training_input, training_target, batch_size=batch_size),\n",
    "              steps_per_epoch=None, shuffle=\"True\", epochs=num_epochs, \n",
    "              workers = 10, max_queue_size = 100, \n",
    "              verbose=2,validation_data=(test_input,test_target),callbacks=[tensorboard_callback])\n",
    "\n",
    "\n",
    "# plot training and validation loss  \n",
    "plt.plot(history.history['loss'], color = 'blue', label = 'training loss')\n",
    "plt.plot(history.history['val_loss'], color = 'red', label = 'validation loss')\n",
    "plt.xlabel('Epoch number')\n",
    "#plt.ylim(0, 1)\n",
    "plt.title('training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# plot training and validation accuracy \n",
    "plt.plot(history.history['categorical_accuracy'], color = 'blue', label = 'training accuracy')\n",
    "plt.plot(history.history['val_categorical_accuracy'], color = 'red', label = 'validation accuracy')\n",
    "plt.xlabel('Epoch number')\n",
    "plt.ylim(0, 1)\n",
    "plt.title('training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# model evaluation\n",
    "train_loss = history.history['loss'][num_epochs-1] \n",
    "train_acc = history.history['categorical_accuracy'][num_epochs-1]\n",
    "validation_loss = model.evaluate(validation_input, validation_target)[0]\n",
    "validation_acc = model.evaluate(validation_input, validation_target)[1]\n",
    "test_loss = model.evaluate(test_input, test_target)[0]\n",
    "test_acc = model.evaluate(test_input, test_target)[1]\n",
    "\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"final training loss: %f\" % train_loss)\n",
    "print(\"final training accuracy: %f\" % train_acc)\n",
    "print(\"final validation loss: %f\" % validation_loss)\n",
    "print(\"final validation accuracy: %f\" % validation_acc)\n",
    "print(\"final test loss: %f\" % test_loss)\n",
    "print(\"final test accuracy: %f\" % test_acc)\n",
    "print(\"\\n\")\n",
    "\n",
    "#plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PhIyWs4tGdq2"
   },
   "source": [
    "\n",
    "**Result for Adam:**\n",
    "\n",
    "**Model configuration:** \n",
    "\n",
    "num_feature_maps_per_layer = [4] # FIX!!!\n",
    "\n",
    "num_hidden_neurons_fully_connected = [100] # FIX!!! \n",
    "\n",
    "kernel_size = 3 # FIX!!!\n",
    "\n",
    "pool_size = 2 # FIX!!!\n",
    "\n",
    "padding = 'same' \n",
    "\n",
    "number of layers: 12\n",
    "\n",
    "final training loss: 1.520513\n",
    "\n",
    "final training accuracy: 0.959700\n",
    "\n",
    "final validation loss: 1.510645\n",
    "\n",
    "final validation accuracy: 0.970400\n",
    "\n",
    "final test loss: 1.511448\n",
    "\n",
    "final test accuracy: 0.969400\n",
    "\n",
    "![IMAGE: lenet](Adam/1.png)\n",
    "\n",
    "![IMAGE: lenet](Adam/1.png)\n",
    "\n",
    "\n",
    "**Findings**\n",
    "\n",
    "Changing batch size and number of features doesnt changes that much but after changing the kernel size  and number of hidden layer upto 180 it shows a massive change in value in .6284 which is almost 30% less than before.\n",
    "\n",
    "![IMAGE: lenet](Adam/1.png)\n",
    "\n",
    "\n",
    "**Result for Adadelta:**\n",
    "\n",
    "num_feature_maps_per_layer = [4] # FIX!!!\n",
    "\n",
    "num_hidden_neurons_fully_connected = [100] # FIX!!! \n",
    "\n",
    "kernel_size = 3 # FIX!!!\n",
    "\n",
    "pool_size = 2 # FIX!!!\n",
    "\n",
    "padding = 'same' \n",
    "\n",
    "\n",
    "number of layers: 12\n",
    "\n",
    "final training loss: 1.540865\n",
    "\n",
    "final training accuracy: 0.944780\n",
    "\n",
    "final validation loss: 1.533213\n",
    "\n",
    "final validation accuracy: 0.954100\n",
    "\n",
    "final test loss: 1.535887\n",
    "\n",
    "final test accuracy: 0.952300\n",
    "\n",
    "![IMAGE: lab5](Adadelta/2.png)\n",
    "\n",
    "![IMAGE: lab5](Adadelta/2.png)\n",
    "\n",
    "**Findings**\n",
    "\n",
    "Changing batch size and number of features doesnt changes that much but after changing the kernel size  and number of hidden layer upto 180 it shows a increase in value of 1%.\n",
    "\n",
    "![IMAGE: lab5](Adadelta/2.png)\n",
    "\n",
    "**Result for Adagrad:**\n",
    "\n",
    "num_feature_maps_per_layer = [4] # FIX!!!\n",
    "\n",
    "num_hidden_neurons_fully_connected = [100] # FIX!!! \n",
    "\n",
    "kernel_size = 3 # FIX!!!\n",
    "\n",
    "pool_size = 2 # FIX!!!\n",
    "\n",
    "padding = 'same' \n",
    "\n",
    "\n",
    "number of layers: 12\n",
    "\n",
    "final training loss: 1.493373\n",
    "\n",
    "final training accuracy: 0.979060\n",
    "\n",
    "final validation loss: 1.490234\n",
    "\n",
    "final validation accuracy: 0.982000\n",
    "\n",
    "final test loss: 1.491791\n",
    "\n",
    "final test accuracy: 0.980600\n",
    "\n",
    "![IMAGE: lab5](Adagrad/2.png)\n",
    "\n",
    "![IMAGE: lab5](Adagrad/2.png)\n",
    "\n",
    "\n",
    "**Findings**\n",
    "\n",
    "Changing batch size and number of features doesnt changes that much but after changing the kernel size  and number of hidden layer upto 180 it shows a slide change in value of 1%.\n",
    "\n",
    "![IMAGE: lab5](Adagrad/2.png)\n",
    "\n",
    "\n",
    "**Result for Nadam:**\n",
    "\n",
    "num_feature_maps_per_layer = [4] # FIX!!!\n",
    "\n",
    "num_hidden_neurons_fully_connected = [100] # FIX!!! \n",
    "\n",
    "kernel_size = 3 # FIX!!!\n",
    "\n",
    "pool_size = 2 # FIX!!!\n",
    "\n",
    "padding = 'same' \n",
    "\n",
    "\n",
    "number of layers: 12\n",
    "\n",
    "final training loss: 1.526426\n",
    "\n",
    "final training accuracy: 0.953620\n",
    "\n",
    "final validation loss: 1.565165\n",
    "\n",
    "final validation accuracy: 0.915600\n",
    "\n",
    "final test loss: 1.565216\n",
    "\n",
    "final test accuracy: 0.914900\n",
    "\n",
    "![IMAGE: lab5](Nadam/2.png)\n",
    "\n",
    "![IMAGE: lab5](Nadam/2.png)\n",
    "\n",
    "**Findings**\n",
    "\n",
    "Changing batch size and number of features doesnt changes that much but after changing the kernel size  and number of hidden layer upto 180 it shows huge change in value of .49.\n",
    "\n",
    "![IMAGE: lab5](Nadam/2.png)\n",
    "\n",
    "**Result fot RMSprop:**\n",
    "\n",
    "num_feature_maps_per_layer = [4] # FIX!!!\n",
    "\n",
    "num_hidden_neurons_fully_connected = [100] # FIX!!! \n",
    "\n",
    "kernel_size = 3 # FIX!!!\n",
    "\n",
    "pool_size = 2 # FIX!!!\n",
    "\n",
    "padding = 'same' \n",
    "\n",
    "number of layers: 12\n",
    "\n",
    "final training loss: 3.046659\n",
    "\n",
    "final training accuracy: 0.129700\n",
    "\n",
    "final validation loss: 3.331042\n",
    "\n",
    "final validation accuracy: 0.248100\n",
    "\n",
    "final test loss: 3.343647\n",
    "\n",
    "final test accuracy: 0.235500\n",
    "\n",
    "![IMAGE: lab5](RMSprop/2.png)\n",
    "\n",
    "![IMAGE: lab5](RMSprop/2.png)\n",
    "\n",
    "**Findings**\n",
    "\n",
    "Changing batch size and number of features doesnt changes that much but after changing the kernel size  and number of hidden layer upto 180 it shows huge change in value of .098 in the graph.\n",
    "\n",
    "![IMAGE: lab5](RMSprop/2.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Comparing with the Previous Assignment 4 section 3:**\n",
    "\n",
    "**Findings for SGD:**\n",
    "\n",
    "Final training accuracy in CNN is much more higher than MLP. In MLP Final training acurecy was .570800 where in CNN it is .98960 and the final test accuracy is also lower than CNN in MLP.\n",
    "\n",
    "\n",
    "**Findings for Adam:**\n",
    "\n",
    "Final training accuracy in CNN is much more higher than MLP. In MLP Final training acurecy was .183700 where in CNN it is .950300 and the final test accuracy is also lower than CNN in MLP.\n",
    "\n",
    "**Findings for Adadelta:**\n",
    "\n",
    "Final training accuracy in CNN is much more higher than MLP. In MLP Final training acurecy was .611920 where in CNN it is .944780 and the final test accuracy is also lower than CNN in MLP leads to .835000.\n",
    "\n",
    "**Findings for Adagrad:**\n",
    "\n",
    "Final training accuracy in CNN is much more higher than MLP. In MLP Final training acurecy was .616440 where in CNN it is .979060 and the final test accuracy is also lower than CNN in MLP leads to .824400.\n",
    "\n",
    "\n",
    "**Findings for Nadam:**\n",
    "\n",
    "Final training accuracy in CNN is much more higher than MLP. In MLP Final training acurecy was .165800 where in CNN it is .953620 and the final test accuracy is also lower than CNN in MLP leads to .102800.\n",
    "\n",
    "\n",
    "\n",
    "**Findings for RMSprop:**\n",
    "\n",
    "Final training accuracy in CNN is much more higher than MLP. In MLP Final training acurecy was .102080 where in CNN it is .129700 and the final test accuracy is also lower than CNN in MLP leads to .161900.\n",
    "\n",
    "**Conclusion:**\n",
    "\n",
    "\n",
    "From my experiment i got CNN for Adam and Nadam solver give more than 80% higer accuracy than the MLP whereas for SGD and Adagrad around 10% higher accuracy than MLP however MLP shows better result for RMSprop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Zf-EinzGdq2"
   },
   "source": [
    "## Exercise 3 (Number of parameters in a fully connected and a convolutional network):\n",
    "\n",
    "a) For a fully connected multi-layer perceptron containing two hidden layers with 100 hidden units each which is designed for the MNIST classification problem, calculate the number of learnable parameters (i.e. parameters which are learned using the backpropagation algorithm).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mGbyFBVTGdq3"
   },
   "source": [
    "### Answer\n",
    "\n",
    "For a fully connected multilayer perceptron containing 2 hidden layers, each hidden layer contains 100 hidden units, and the output layer with 10 possible output,the number of learnable parametres in this case the learnable waits will be as follows\n",
    "\n",
    "let X be the size of input. Thus\n",
    "\n",
    "Number of learbable parameters in input layer to 1st hidden layer = number of weights + number of bias\n",
    "                                         \n",
    "                                            =(X*100) + 100\n",
    "                                            = 100X+100\n",
    "\n",
    "Number of learbable parameters in 1st hidden layer to 2nd hidden layer = number of weights + number of bias\n",
    "                                         \n",
    "                                            =(100*100) + 100\n",
    "                                            = 10100\n",
    "\n",
    "Number of learbable parameters in 1st hidden layer to 2nd hidden layer = number of weights + number of bias\n",
    "                                         \n",
    "                                            =(100*10) + 10\n",
    "                                            = 1010\n",
    "\n",
    "**The number of total learnable parameters = 100X+100+10100+1100 = 100X+11210**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BAwBKMGsGdq4"
   },
   "source": [
    "b)\tThe figure shows the architecture of the famous LeNet-5 convolutional network. Calculate the number of trainable parameters and the number of connections (synaptic weights plus biases, i.e. in augmented space). \n",
    "Cx: Convolution layer x, Sx: Subsampling layer x, Fx: Fully connected layer x\n",
    "\n",
    "![IMAGE: lenet](images/LeNet5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xqHmj4m6Gdq4"
   },
   "source": [
    "### Answer\n",
    "\n",
    "**Number of trainable parameters and connections**\n",
    "\n",
    "1st convolutional layer (C1)\n",
    "\n",
    "In this layer the input image is turned into 6 feature maps each having the dimension (5*5).\n",
    "\n",
    "*Number of trainable parametres = number of weights + number of bias*          \n",
    "\n",
    "                                   = (5*5*6) + 6 = 156\n",
    "\n",
    "*Number of connections* = 28 * 28 * 156 = 122304\n",
    "\n",
    "2nd sub-sampling layer (S2)\n",
    "\n",
    "In this layer sub-sampling is applied and the filter is modified into 6 layers with dimension (2*2)\n",
    "\n",
    "*Number of trainable parameters = (number of coeffecient + number of bias)* * *number of filters)*\n",
    "\n",
    "                                    = (1+1)*6 = 12\n",
    "\n",
    "*Number of connections* = 14 * 14 * 30 = 5880\n",
    "\n",
    "3rd convolution layer (C3)\n",
    "\n",
    "In this layer there are 16 feature maps with dimension (5*5) among which only 10 is connected with the previous layer.\n",
    "\n",
    "*Number of trainable parameter = number of weights + number of bias*\n",
    "\n",
    "                                  = (5*5*6*10) + 16 = 1516\n",
    "\n",
    "*Number of connections* = 10 * 10 * 1516 = 151600\n",
    "\n",
    "4th sub-sampling layer (S4)\n",
    "\n",
    "In this layer sub-sampling is applied and the filter is modified into 16 layers with dimension (2*2)\n",
    "\n",
    "*Number of trainable parameters = (number of coeffecient + number of bias)* * *number of filters)*\n",
    "\n",
    "                                    = (1+1)*16 = 32\n",
    "5th convolution layer (C5)\n",
    "\n",
    "The is a fully connected convolutional layer with 120 feature maps with dimension 1×1. Each of the 120 units in this layer is connected to all the 400 nodes (5x5x16) in the fourth layer S4.\n",
    "\n",
    "*Number of trainable parameter = number of weights + number of bias*\n",
    "\n",
    "                                  = (5*5*16*120) + 120 = 48120\n",
    "\n",
    "*Number of connections* = 5 * 5 * 80 = 2000\n",
    "\n",
    "\n",
    "6th layer (F6)\n",
    "\n",
    "This is a fully connected layer with 84 units.\n",
    "\n",
    "*Number of trainable parameter = number of weights + number of bias*\n",
    "\n",
    "                                  = (84*120) + 84 = 10164\n",
    "\n",
    "Output layer\n",
    "\n",
    "This layer produces 10 possible output.\n",
    "\n",
    "*Number of trainable parameter = number of weights + number of bias*\n",
    "\n",
    "                                  = (84*120) + 84 = 10164\n",
    "\n",
    "**Total number of trainable parameters in all layers = 156+12+1516+32+48120+10164 = 60000**\n",
    "\n",
    "**Total number of connections in all layers = 2000+151600+5880+122304 = 281784**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UsA5Sb5ZGdq5"
   },
   "source": [
    "c) (optional) Calculate the output dimensions and number of parameters of the AlexNet without grouping (the division into two separate paths), i.e. when the AlexNet is realized in a single path."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MrnxTuOhGdq6"
   },
   "source": [
    "### Answer\n",
    "\n",
    "Write your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BFtxnDisGdq6"
   },
   "source": [
    "d) (optional) How do these numbers change in the ZF net?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QH9zNHijGdq7"
   },
   "source": [
    "### Answer\n",
    "\n",
    "Write your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ccT-j_5GGdq7"
   },
   "source": [
    "# Exercise 4 (Overfitting - CNN on CIFAR-10):\n",
    "\n",
    "The jupyter notebook provides code to train a network to classify images of the CIFAR-10 dataset (https://www.cs.toronto.edu/~kriz/cifar.html). Training is performed on a subset of 10000 images from the dataset. Run the code and discuss the results. Then, explore possibilities to counteract the observed behavior.\n",
    "\n",
    "Remarks: \n",
    "\n",
    "1)\tIf you perform the experiments in Colab, you may explicitly select GPU execution: Select the “Edit” tab in Colab, then choose “Notebook settings”, and then select “GPU” as hardware accelerator. See also the accompanying documentation in Colab.\n",
    "\n",
    "2)\tFor Tensorboard to work, you have to explicitly activate cookies from all third parties in your browser. Furthermore, it has been found that Tensorboard could not be run successfully in all configuration or browsers. If it doesn’t work in your configuration (e.g. error “Google 403. That’s an error. That’s all we know.”) please outcomment the lines referring to Tensorboard.\n",
    "\n",
    "3)\tExperiments with data augmentation may substantially increase runtime! Further information on data augmentation in Keras can be found at https://keras.io/preprocessing/image/.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5tAF2lCIJ_6_"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout, Activation, BatchNormalization, GaussianNoise\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.optimizers import SGD, Adam, Adadelta, Adagrad, Nadam, RMSprop, schedules\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "import tensorflow.keras.initializers as tfi\n",
    "import tensorflow.keras.regularizers as tfr\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.utils import shuffle\n",
    "import os\n",
    "import datetime\n",
    "import sys\n",
    "\n",
    "###--------\n",
    "# load data\n",
    "###--------\n",
    "\n",
    "(training_images, training_target), (test_images, test_target) = cifar10.load_data()\n",
    "training_target = np.reshape(training_target, training_target.shape[0])\n",
    "test_target = np.reshape(test_target, test_target.shape[0])\n",
    "                     \n",
    "# training_images: shape (50000, 32, 32, 3)\n",
    "# training_target: shape (50000, 1)\n",
    "# test_images:  shape (10000, 32, 32, 3)\n",
    "# test_target:  shape (10000, 1)\n",
    "\n",
    "# shuffle data\n",
    "training_images, training_target = shuffle(training_images, training_target)\n",
    "test_images, test_target = shuffle(test_images, test_target)\n",
    "\n",
    "# plot some sample images\n",
    "num_examples = 3\n",
    "for s in range(num_examples):\n",
    "  print(\"Example image, true label: %d\" % training_target[s])\n",
    "  plt.imshow(training_images[s])\n",
    "  plt.show()\n",
    "\n",
    "num_classes = 10 # CIFAR-10 has 10 classes\n",
    "\n",
    "print(\"min. training data: %f\" % np.min(training_images))\n",
    "print(\"max. training data: %f\" % np.max(training_images))\n",
    "print(\"min. test data: %f\" % np.min(test_images))\n",
    "print(\"max. test data: %f\" % np.max(test_images))\n",
    "\n",
    "# scaling inputs from range 0 ... 255 to range [0,1] if desired\n",
    "min_val = np.min(training_images) # 0\n",
    "max_val = np.max(training_images) # 255\n",
    "training_images = training_images.astype('float32')\n",
    "test_images = test_images.astype('float32')\n",
    "training_input = (training_images - min_val) / (max_val - min_val)\n",
    "test_input = (test_images) / (max_val - min_val)\n",
    "\n",
    "# use only part of training set\n",
    "numTrainingExamplesUsed = 10000 # FIX!!!\n",
    "print(\"USING ONLY %d TRAINING EXAMPLES\" % numTrainingExamplesUsed)\n",
    "training_input = training_input[:numTrainingExamplesUsed]\n",
    "training_target = training_target[:numTrainingExamplesUsed]\n",
    "\n",
    "# histograms of input values\n",
    "nBins = 100\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15,10))\n",
    "axes[0].hist(training_input.flatten(), nBins)\n",
    "axes[0].set_xlabel(\"training\")\n",
    "axes[0].set_ylabel(\"counts\")\n",
    "axes[0].set_ylim((0,0.3*1e7))\n",
    "\n",
    "axes[1].hist(test_input.flatten(), nBins)\n",
    "axes[1].set_xlabel(\"test\")\n",
    "axes[1].set_ylabel(\"counts\")\n",
    "axes[1].set_ylim((0,0.3*1e7))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# histogram of training labels\n",
    "nBins = num_classes\n",
    "plt.hist(training_target, nBins)\n",
    "plt.xlabel(\"classes\")\n",
    "plt.ylabel(\"counts\")\n",
    "plt.title(\"histogram of training class labels\")\n",
    "plt.show()\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "training_target = tf.keras.utils.to_categorical(training_target, num_classes)\n",
    "test_target = tf.keras.utils.to_categorical(test_target, num_classes)\n",
    "\n",
    "###-----------\n",
    "# define model\n",
    "###-----------\n",
    "\n",
    "num_classes = 10 # CIFAR-10 has 10 classes\n",
    "num_outputs = num_classes\n",
    "learningRate = 0.001 \n",
    "\n",
    "def schedule(epoch):\n",
    "    lrate = 0.001 # initial learning rate\n",
    "    if epoch > 75:\n",
    "        lrate = 0.0005\n",
    "    if epoch > 100:\n",
    "        lrate = 0.0003\n",
    "    return lrate\n",
    "\n",
    "activation = 'elu' # exponential linear unit \n",
    "\n",
    "dropout = 0.5 # 0 if no dropout, else fraction of dropout units in fully connected layers (e.g. 0.5)   # FIX!!!\n",
    "batch_normalization = True #  FIX!!!\n",
    "stddev = 0.2 # 0 for not adding Gaussian noise; otherwise, initial stddev for adding Gaussian noise in convolutional layers (e.g. 0.2) # FIX!!!\n",
    "\n",
    "weight_init = tfi.glorot_uniform() # default: glorot_uniform(); e.g. glorot_normal(), he_normal(), he_uniform(), lecun_normal(), lecun_uniform(), RandomNormal(), RandomUniform(), Zeros() etc.\n",
    "bias_init = tfi.Zeros() # default: Zeros(); for some possible values see weight initializers\n",
    "\n",
    "regularization_weight = 0.0001 # 0 for no regularization or e.g. 0.0001 to apply regularization # FIX!!!\n",
    "regularizer = tfr.l2(l=regularization_weight) # or l2 or l1_l2; used for both weights and biases # FIX!!!\n",
    "\n",
    "num_epochs = 125  # FIX !!! \n",
    "batch_size = 64 # FIX !!!  \n",
    "\n",
    "data_augmentation = True # FIX !!!\n",
    "\n",
    "# Sequential network structure.\n",
    "num_feature_maps_per_layer = [32, 64, 128] # FIX!!!\n",
    "num_hidden_neurons_fully_connected = [] # FIX!!! \n",
    "kernel_size = (3, 3)\n",
    "pool_size = (2, 2)\n",
    "padding = 'same'\n",
    "\n",
    "experiment_name = \"\"\n",
    "#experiment_name = \"dropout%s-ep%d-bsize%d-data%0.02f-normal%s\" %\\\n",
    "#  (\"T\" if dropout else \"F\", \n",
    "#  nb_epoch_count,\n",
    "#  bsize,\n",
    "#  percent_data,\n",
    "#  \"T\" if dropout else \"F\", )\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "if len(num_feature_maps_per_layer) == 0:\n",
    "  print(\"Error: Must at least have one convolutional layer!\")\n",
    "  sys.exit()  \n",
    "\n",
    "# add first convolutional layer connecting to input layer\n",
    "num_feature_maps = num_feature_maps_per_layer[0]\n",
    "model.add(Conv2D(filters=num_feature_maps, kernel_size=kernel_size, input_shape=training_images.shape[1:], padding=padding))\n",
    "if batch_normalization:\n",
    "  model.add(BatchNormalization())\n",
    "model.add(Activation(activation)) \n",
    "model.add(Conv2D(filters=num_feature_maps, kernel_size=kernel_size, padding=padding))\n",
    "if batch_normalization:\n",
    "  model.add(BatchNormalization())\n",
    "model.add(Activation(activation))  \n",
    "model.add(MaxPooling2D(pool_size=pool_size))\n",
    "model.add(Dropout(0.2)) # normally not used\n",
    "if stddev > 0:\n",
    "  print(\"First convolutional layer: Adding additive Gaussian noise with stddev %f\" % stddev)\n",
    "  model.add(GaussianNoise(stddev=stddev)) \n",
    "\n",
    "# potentially further hidden layers\n",
    "for i in range(1, len(num_feature_maps_per_layer)):\n",
    "  # add convolutional layers with corresponding number of feature maps\n",
    "  num_feature_maps = num_feature_maps_per_layer[i] \n",
    "  model.add(Conv2D(filters=num_feature_maps, kernel_size=kernel_size, padding=padding))\n",
    "  if batch_normalization:\n",
    "    model.add(BatchNormalization())\n",
    "  model.add(Activation(activation))   \n",
    "  model.add(Conv2D(filters=num_feature_maps, kernel_size=kernel_size, activation=activation, padding=padding))\n",
    "  if batch_normalization:\n",
    "    model.add(BatchNormalization())\n",
    "  model.add(Activation(activation))   \n",
    "  model.add(MaxPooling2D(pool_size=pool_size))\n",
    "  if stddev > 0:\n",
    "    stddev = stddev + 0.1 # increase value of stddev with each convolutional block\n",
    "    print(\"Convolutional layer %d: Adding additive Gaussian noise with stddev %f\" % (i, stddev))\n",
    "    model.add(GaussianNoise(stddev=stddev)) \n",
    "    model.add(Dropout(0.2 + i/10.0)) # normally not used\n",
    "\n",
    "# fully connected layers\n",
    "model.add(Flatten())\n",
    "for i in range(len(num_hidden_neurons_fully_connected)):\n",
    "  # add fully connected layers with corresponding number of hidden neurons\n",
    "  num_hidden = num_hidden_neurons_fully_connected[i]\n",
    "  model.add(Dense(num_hidden, activation=activation))\n",
    "  if dropout:\n",
    "    model.add(Dropout(dropout))\n",
    "  if batch_normalization:\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "# output layer\n",
    "model.add(Dense(units=num_outputs, name = \"output\", activation = 'softmax', kernel_initializer=weight_init, bias_initializer = bias_init, kernel_regularizer=regularizer, bias_regularizer=regularizer))\n",
    "  \n",
    "# print configuration\n",
    "print(\"\\nModel configuration: \")\n",
    "print(model.get_config())\n",
    "print(\"\\n\")\n",
    "print(\"... number of layers: %d\" % len(model.layers))\n",
    "\n",
    "# show how the model looks\n",
    "model.summary()\n",
    "      \n",
    "# compile model with RMSprop optimizer\n",
    "opt = RMSprop(learning_rate=learningRate, decay=1e-6)\n",
    "model.compile(optimizer=opt,loss=tf.keras.losses.CategoricalCrossentropy(),metrics=['accuracy'])\n",
    "\n",
    "###----------\n",
    "# train model\n",
    "###----------\n",
    "\n",
    "# prepare tensorboard\n",
    "logs_base_dir = \"./logs\"\n",
    "os.makedirs(logs_base_dir, exist_ok=True)\n",
    "logdir = os.path.join(logs_base_dir, experiment_name, datetime.datetime.now().strftime(\"%d-%m-%H:%M:%S\"))\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "lr_callback = tf.keras.callbacks.LearningRateScheduler(schedule)\n",
    "\n",
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    history = model.fit(training_input, training_target, epochs=num_epochs, \n",
    "                        batch_size=batch_size, shuffle=\"True\", verbose=2, \n",
    "                        validation_data=(test_input, test_target), callbacks=[tensorboard_callback, lr_callback])\n",
    "else:\n",
    "    #data augmentation\n",
    "    print('Using data augmentation.')\n",
    "    datagen = ImageDataGenerator(\n",
    "       rotation_range=15, \n",
    "       width_shift_range=0.1,\n",
    "       height_shift_range=0.1,\n",
    "       horizontal_flip=True,\n",
    "       )\n",
    "    datagen.fit(training_input)\n",
    "\n",
    "    model.fit(datagen.flow(training_input, training_target, batch_size=batch_size),\n",
    "              steps_per_epoch=None, shuffle=\"True\", epochs=num_epochs, \n",
    "              workers = 10, max_queue_size = 100, \n",
    "              verbose=2,validation_data=(test_input,test_target),callbacks=[tensorboard_callback, lr_callback])\n",
    "\n",
    "# plot training and validation loss  \n",
    "plt.plot(history.history['loss'], color = 'blue', label = 'training loss')\n",
    "plt.plot(history.history['val_loss'], color = 'red', label = 'validation loss')\n",
    "plt.xlabel('Epoch number')\n",
    "#plt.ylim(0, 1)\n",
    "plt.title('training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# plot training and validation accuracy \n",
    "plt.plot(history.history['accuracy'], color = 'blue', label = 'training accuracy')\n",
    "plt.plot(history.history['val_accuracy'], color = 'red', label = 'validation accuracy')\n",
    "plt.xlabel('Epoch number')\n",
    "plt.ylim(0, 1)\n",
    "plt.title('training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# model evaluation\n",
    "train_loss = history.history['loss'][num_epochs-1] \n",
    "train_acc = history.history['accuracy'][num_epochs-1]\n",
    "test_loss = model.evaluate(test_input, test_target)[0]\n",
    "test_acc = model.evaluate(test_input, test_target)[1]\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"final training loss: %f\" % train_loss)\n",
    "print(\"final training accuracy: %f\" % train_acc)\n",
    "print(\"final test loss: %f\" % test_loss)\n",
    "print(\"final test accuracy: %f\" % test_acc)\n",
    "print(\"\\n\")\n",
    "\n",
    "# execute tensorboard\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir {logs_base_dir}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R4K4Fz_qGdrB"
   },
   "source": [
    "As before, you can visualize your different runs via Tensorboard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_CV5tfOiXpkb"
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir {logs_base_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z4DXg1ROGdrG"
   },
   "source": [
    "If you want to download the created logs you can create a zip file with the following command. Remember that any saved data in Colab will be lost after a reboot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WbWKQwJD11Li"
   },
   "outputs": [],
   "source": [
    "# create zip from logs\n",
    "!zip -r ./logs.zip ./logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i-BHN8xXGdrK"
   },
   "source": [
    "### Answer\n",
    "**Some common properties**\n",
    "\n",
    "activation = 'elu'\n",
    "\n",
    "num_epochs = 125  # FIX !!! \n",
    "\n",
    "batch_size = 64 # FIX !!!\n",
    "\n",
    "batch_normalization = True #\n",
    "\n",
    "**model : #1**\n",
    "=====================\n",
    "dropout = 0 \n",
    "\n",
    "data_augmentation = False\n",
    "\n",
    "**Result**\n",
    "\n",
    "final training loss: 0.000000\n",
    "\n",
    "final training accuracy: 1.000000\n",
    "\n",
    "final test loss: 3.196528\n",
    "\n",
    "final test accuracy: 0.716400\n",
    "\n",
    "**model : #2**\n",
    "=====================\n",
    "dropout = 0.5\n",
    "\n",
    "data_augmentation = False\n",
    "\n",
    "**Result**\n",
    "\n",
    "final training loss: 0.000000\n",
    "\n",
    "final training accuracy: 1.000000\n",
    "\n",
    "final test loss: 3.224531\n",
    "\n",
    "final test accuracy: 0.714800\n",
    "\n",
    "**model : #3**\n",
    "=====================\n",
    "dropout = 0.5 \n",
    "\n",
    "stddev = 0.2\n",
    "\n",
    "model.add(Dropout(0.2 + i/10.0))\n",
    "\n",
    "data_augmentation = False # FIX !!!\n",
    "\n",
    "**Result**\n",
    "\n",
    "final training loss: 0.036017\n",
    "\n",
    "final training accuracy: 0.986900\n",
    "\n",
    "final test loss: 1.638021\n",
    "\n",
    "final test accuracy: 0.776500\n",
    "\n",
    "**model : #4**\n",
    "=====================\n",
    "\n",
    "dropout = 0.5\n",
    "\n",
    "stddev = 0.2\n",
    "\n",
    "data_augmentation = True # FIX !!!\n",
    "\n",
    "**Result**\n",
    "\n",
    "final training loss: 0.036017\n",
    "\n",
    "final training accuracy: 0.986900\n",
    "\n",
    "final test loss: 0.960418\n",
    "\n",
    "final test accuracy: 0.808000\n",
    "\n",
    "**model : #5**\n",
    "=====================\n",
    "\n",
    "dropout = 0 \n",
    "\n",
    "stddev = 0\n",
    "\n",
    "data_augmentation = True # FIX !!!\n",
    "\n",
    "**Result**\n",
    "\n",
    "final training loss: 0.036017\n",
    "\n",
    "final training accuracy: 0.986900\n",
    "\n",
    "final test loss: 1.029311\n",
    "\n",
    "final test accuracy: 0.802700\n",
    "\n",
    "**model : #6**\n",
    "=====================\n",
    "\n",
    "dropout = 0.5 \n",
    "\n",
    "stddev = 0.2 \n",
    "\n",
    "regularization_weight = 0.0001 \n",
    "\n",
    "data_augmentation = True # FIX !!!\n",
    "\n",
    "**Result**\n",
    "\n",
    "final training loss: 0.036017\n",
    "\n",
    "final training accuracy: 0.986900\n",
    "\n",
    "final test loss: 0.954820\n",
    "\n",
    "final test accuracy: 0.816200\n",
    "\n",
    "\n",
    "**model : #7**\n",
    "=====================\n",
    "\n",
    "dropout = 0.5 \n",
    "\n",
    "stddev = 0.2 \n",
    "\n",
    "regularization_weight = 0.0001 \n",
    "\n",
    "data_augmentation = True # FIX !!!\n",
    "\n",
    "model.add(Dropout(0.2 + i/10.0)) # normally not used\n",
    "\n",
    "**Result**\n",
    "\n",
    "final training loss: 0.036017\n",
    "\n",
    "final training accuracy: 0.986900\n",
    "\n",
    "final test loss: 0.732826\n",
    "\n",
    "final test accuracy: 0.805500\n",
    "\n",
    "**model : #8**\n",
    "=====================\n",
    "\n",
    "learning rate = 0.01\n",
    "\n",
    "dropout = 0.5 \n",
    "\n",
    "stddev = 0.2 \n",
    "\n",
    "regularization_weight = 0.0001 \n",
    "\n",
    "data_augmentation = True # FIX !!!\n",
    "\n",
    "**Result**\n",
    "\n",
    "final training loss: 0.036017\n",
    "\n",
    "final training accuracy: 0.986900\n",
    "\n",
    "final test loss: 0.732826\n",
    "\n",
    "final test accuracy: 0.805500\n",
    "\n",
    "**model : #9**\n",
    "=====================\n",
    "\n",
    "dropout = 0.5 \n",
    "stddev = 0.2 \n",
    "\n",
    "regularization_weight = 0.0001 \n",
    "\n",
    "data_augmentation = True # FIX !!!\n",
    "\n",
    "**Result**\n",
    "\n",
    "final training loss: 0.036017\n",
    "\n",
    "final training accuracy: 0.986900\n",
    "\n",
    "final test loss: 0.687224\n",
    "\n",
    "final test accuracy: 0.812400\n",
    "\n",
    "\n",
    "Findings\n",
    "=============\n",
    "* Almost all models are overfitting  \n",
    "* The model **#6** and **#9** are not that overfitting but I thing not acceptable  \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Sheet5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
