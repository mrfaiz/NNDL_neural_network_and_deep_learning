{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MOitTdg3yP-M"
   },
   "source": [
    "# Important Information\n",
    "\n",
    "This file has two usages. \n",
    "1. This file can be displayed in Jupyter. You can read the task and insert your answers here. Start the notebook from an Anaconda prompt and change to the working directory containing the *.ipynb file.\n",
    "2. You can execute the code in Google Colab making use of Keras and Tensorflow. If you do not want to create a Google Account, you have to create a local environment for Keras and Tensorflow.\n",
    "\n",
    "For submission, convert your notebook with your solutions and all images to a pdf-file and upload this file into OLAT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-CXSNwExJb1C"
   },
   "source": [
    "Setup for this exercise sheet. Download data and define Tensorflow version.\n",
    "Execute code only if you setup your enviroment correctly or if you are inside a colab enviroment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 129
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Q7aTStJHJaay",
    "outputId": "7cd84744-a02f-4db8-ade2-21c81f6c4039",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! git clone https://gitlab+deploy-token-26:XBza882znMmexaQSpjad@git.informatik.uni-kiel.de/las/nndl.git\n",
    "\n",
    "%tensorflow_version 2.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1 (Learning in neural networks)\n",
    "\n",
    "a) Explain the following terms related to neural networks as short and precise as possible. \n",
    "\n",
    "* Loss function\n",
    "* Stochastic gradient descent\n",
    "* Mini-batch \n",
    "* Regularization\n",
    "* Dropout\n",
    "* Batch normalization\n",
    "* Learning with momentum\n",
    "* Data augmentation\n",
    "* Unsupervised pre-training / supervised fine-tuning\n",
    "* Deep learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Answer: Write your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d8ahoTHxBW82"
   },
   "source": [
    "b) Name the most important output activation functions f(z), i.e., activation function of the output neuron(s), together with a corresponding suitable loss function L (in both cases, give the mathematical equation). Indicate whether such a perceptron is used for a classification or a regression task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Answer: Write your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2 (Multi-layer perceptron: Backprogagation, regression problem)\n",
    "a) Consider the multi-layer perceptron in the following figure: \n",
    "\n",
    "![IMAGE: perceptron](images/MLP.png)\n",
    "\n",
    "The activation function at all hidden nodes is ReLU and at the output node linear. \n",
    "\n",
    "Perform one iteration of plain backpropagation (without momentum, regularization etc.), based on a mini-batch composed of two input samples ${x}^{(\\mu)}$ with corresponding target values $y^{(\\mu)}$, learning rate $\\eta$ and SSE loss:\n",
    "\n",
    "${x}^{(1)}=(-1,1)^T$ with target $y^{(1)}=1$ and ${x}^{(2)}=(2,-1)^T$ with target $y^{(2)}=-1$  \n",
    "\n",
    "The initial weights and biases are given as ($t$ is the iteration index):\n",
    "\n",
    "${W}^1(t=0)=\\begin{bmatrix} 1 & 2 \\\\ 0 & -1 \\\\ -1 & -3 \\\\ -2 & 2 \\end{bmatrix}$; \n",
    "${W}^2(t=0)=\\begin{bmatrix} 1 & 0 & -1 & 2 \\end{bmatrix}$\n",
    "\n",
    "${b}^1(t=0)=\\begin{bmatrix} -2 \\\\ 2 \\\\ 0 \\\\-2 \\end{bmatrix}$;\n",
    "${b}^2(t=0)=-2$\n",
    "\n",
    "For the forward path, calculate the postsynaptic potential (PSP), the activations and outputs and insert them into the following table:\n",
    "\n",
    "Input<br> ${x}=(x_1, x_2)^T = a^0$ | PSP<br> $z^1$ | Activation<br> $a^1$ |  Ouput<br>$\\hat{y}=a^2$ \n",
    ":--------:|:--------:|:--------:|:--------:\n",
    "$(-1, 1)^T$ |   | | |  \n",
    "$(2, -1)^T$ |   | | |  \n",
    " \n",
    "For the backward path, calculate the updated weights and biases for the hidden and output layer and insert them into the following table:\n",
    "\n",
    "Weights<br> $W^1(t=1)$ | Bias<br> $b^1(t=1)$ | Weights<br> $W^2(t=1)$ |  Bias<br> $b^2(t=1)$ \n",
    ":---:|:---:|:---:|:---:\n",
    " |   | | |  \n",
    " |   | | | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dHDU9m2hChSn"
   },
   "source": [
    "b) The goal of this exercise is to train a multi-layer perceptron to solve a high difficulty level nonlinear regression problem. The data has been generated using an exponential function with the following shape:\n",
    "\n",
    "![IMAGE: perceptron](images/Eckerle4Dataset.png)\n",
    "\n",
    "This graph corresponds to the values of a dataset that can be downloaded from the Statistical Reference Dataset of the Information Technology Laboratory of the United States on this link:\n",
    "http://www.itl.nist.gov/div898/strd/nls/data/eckerle4.shtml\n",
    "\n",
    "This dataset is provided in the file Eckerle4.csv. Note that this dataset is divided into a training and test corpus comprising 60% and 40% of the data samples, respectively. Moreover, the input and output values are normalized to the interval [0, 1]. Basic code to load the dataset and divide it into a training and test corpus, normalizing the data and to apply a multi-layer perceptron is provided in the Jupyter notebook.\n",
    "\n",
    "Choose a suitable network topology (number of hidden layers and hidden neurons, potentially include dropout, activation function of hidden layers) and use it for the multi-layer perceptron defined in the Jupyter notebook. Set further parameters (learning rate, loss function, optimizer, number of epochs, batch size; see the lines marked with *# FIX!!!* in the Jupyter notebook). Try to avoid underfitting and overfitting. Vary the network and parameter configuration in order to achieve a network performance as optimal as possible. For each network configuration, due to the random components in the experiment, perform (at least) 4 different training and evaluation runs and report the mean and standard deviation of the training and evaluation results. Report on your results and conclusions.\n",
    "\n",
    "(Source of exercise: http://gonzalopla.com/deep-learning-nonlinear-regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from os.path import join\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
    "from tensorflow.keras import Model, Input, Sequential\n",
    "from tensorflow.keras.optimizers import SGD, Adam, Adadelta, Adagrad, Nadam, RMSprop\n",
    "from tensorflow.keras.utils import normalize\n",
    "import pandas\n",
    "from sklearn import preprocessing\n",
    "from sklearn import model_selection\n",
    "import sys\n",
    "\n",
    "###--------\n",
    "# load data\n",
    "###--------\n",
    "\n",
    "# Imports csv into pandas DataFrame object.\n",
    "path_to_task = \"nndl/Lab4\"\n",
    "Eckerle4_df = pandas.read_csv(join(path_to_task,\"Eckerle4.csv\"), header=0)\n",
    " \n",
    "# Converts dataframes into numpy objects.\n",
    "Eckerle4_dataset = Eckerle4_df.values.astype(\"float32\")\n",
    "# Slicing all rows, second column...\n",
    "X = Eckerle4_dataset[:,1]\n",
    "# Slicing all rows, first column...\n",
    "y = Eckerle4_dataset[:,0]\n",
    " \n",
    "# plot data\n",
    "plt.plot(X,y, color='red')\n",
    "plt.legend(labels=[\"data\"], loc=\"upper right\")\n",
    "plt.title(\"data\")\n",
    "plt.show()\n",
    "\n",
    "###-----------\n",
    "# process data\n",
    "###-----------\n",
    "\n",
    "# Data Scaling from 0 to 1, X and y originally have very different scales.\n",
    "X_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "y_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "X_scaled = ( X_scaler.fit_transform(X.reshape(-1,1)))\n",
    "y_scaled = (y_scaler.fit_transform(y.reshape(-1,1)).reshape(-1) )\n",
    " \n",
    "# Preparing test and train data: 60% training, 40% testing.\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split( X_scaled, y_scaled, test_size=0.40, random_state=3)\n",
    "\n",
    "\n",
    "###-----------\n",
    "# define model\n",
    "###-----------\n",
    "\n",
    "num_inputs = X_train.shape[1] # should be 1 in case of Eckerle4\n",
    "num_hidden = ... # for each hidden layer: number of hidden units in form of a python list   # FIX!!!\n",
    "num_outputs = 1 # predict single number in case of Eckerle4\n",
    "\n",
    "activation = '...' # activation of hidden layers   # FIX!!!\n",
    "dropout = ... # 0 if no dropout, else fraction of dropout units (e.g. 0.2)   # FIX!!!\n",
    "\n",
    "# Sequential network structure.\n",
    "model = Sequential()\n",
    "\n",
    "if len(num_hidden) == 0:\n",
    "  print(\"Error: Must at least have one hidden layer!\")\n",
    "  sys.exit()  \n",
    "\n",
    "# add first hidden layer connecting to input layer\n",
    "model.add(Dense(num_hidden[0], input_dim=num_inputs, activation=activation))\n",
    "\n",
    "if dropout: \n",
    "  # dropout of fraction dropout of the neurons and activation layer.\n",
    "  model.add(Dropout(dropout))\n",
    "  # model.add(Activation(\"linear\"))\n",
    "\n",
    "# potentially further hidden layers\n",
    "for i in range(1, len(num_hidden)):\n",
    "  # add hidden layer with len[i] neurons\n",
    "  model.add(Dense(num_hidden[i], activation=activation))\n",
    "#  model.add(Activation(\"linear\"))\n",
    "\n",
    "  if dropout:\n",
    "  # dropout of fraction dropout of the neurons and activation layer.\n",
    "    model.add(Dropout(dropout))\n",
    "  #  model.add(Activation(\"linear\"))\n",
    "\n",
    "# output layer\n",
    "model.add(Dense(1))\n",
    "\n",
    "# show how the model looks\n",
    "model.summary()\n",
    "\n",
    "# compile model\n",
    "opt = ... # FIX!!!\n",
    "model.compile(loss='...', optimizer=opt, metrics=[\"...\"])# FIX!!!\n",
    "\n",
    "# Training model with train data. Fixed random seed:\n",
    "np.random.seed(3)\n",
    "num_epochs = ...   # FIX !!!\n",
    "batch_size = ... # FIX !!! \n",
    "history = model.fit(X_train, y_train, epochs=num_epochs, batch_size=batch_size, verbose=2)\n",
    "\n",
    "###-----------\n",
    "# plot results\n",
    "###-----------\n",
    "\n",
    "print(\"final (mse) training error: %f\" % history.history['loss'][num_epochs-1])\n",
    "\n",
    "plt.plot(history.history['loss'], color='red', label = 'training loss')\n",
    "plt.legend(labels=[\"loss\"], loc=\"upper right\")\n",
    "plt.title(\"training (mse) error\")\n",
    "plt.show()\n",
    "\n",
    "# Plot in blue color the predicted data and in green color the\n",
    "# actual data to verify visually the accuracy of the model.\n",
    "predicted = model.predict(X_test)\n",
    "plt.plot(y_scaler.inverse_transform(predicted.reshape(-1,1)), color=\"blue\")\n",
    "plt.plot(y_scaler.inverse_transform(y_test.reshape(-1,1)), color=\"green\")\n",
    "plt.legend(labels=[\"predicted\", \"target\"], loc=\"upper right\")\n",
    "plt.title(\"evaluation on test corpus\")\n",
    "plt.show()\n",
    "print(\"test error: %f\" % model.evaluate(X_test, y_test)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Answer: Write your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V0dZkCh_Cm2n"
   },
   "source": [
    "# Exercise 3 (Parameters of a multi-layer perceptron – digit recognition)\n",
    "\n",
    "In the following exercises, we use Tensorflow and Keras to configure, train and apply a multi-layer perceptron to the problem of recognizing handwritten digits (the famous “MNIST” problem). The MNIST data are loaded using a Tensorflow Keras built-in function.\n",
    "\n",
    "Perform experiments on this pattern recognition problem trying to investigate the influence of a number of parameters on the classification performance. This may refer to\n",
    "\n",
    "-\tthe learning rate and potentially learning schedule,\n",
    "-\tthe number of hidden neurons (in a network with a single hidden layer),\n",
    "-\tthe number of hidden layers as well as applying dropout and / or batch normalization,\n",
    "-\tthe solver (including momentum),\n",
    "-\tthe activation function at hidden layers,\n",
    "-\tregularization.\n",
    "\n",
    "The script in the Jupyter notebook can serve as a basis or starting point.\n",
    "\n",
    "Report your findings and conclusions.\n",
    "\n",
    "**Note: These experiments may require a lot of computation time!**\n",
    "\n",
    "**Further investigations and experiments as well as code extensions and modifications are welcome!**\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from os.path import join\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, BatchNormalization\n",
    "from tensorflow.keras import Model, Input, Sequential\n",
    "from tensorflow.keras.optimizers import SGD, Adam, Adadelta, Adagrad, Nadam, RMSprop, schedules\n",
    "from tensorflow.keras.utils import normalize\n",
    "import tensorflow.keras.datasets as tfds\n",
    "import tensorflow.keras.initializers as tfi\n",
    "import tensorflow.keras.regularizers as tfr\n",
    "\n",
    "###--------\n",
    "# load data\n",
    "###--------\n",
    "\n",
    "(training_input, training_target), (test_input, test_target)  = tfds.mnist.load_data()\n",
    "\n",
    "# Reserve 10,000 samples for validation\n",
    "validation_input = training_input[-10000:]\n",
    "validation_target = training_target[-10000:]\n",
    "training_input = training_input[:-10000]\n",
    "training_target = training_target[:-10000]\n",
    "\n",
    "print(\"training input shape: %s, training target shape: %s\"  % (training_input.shape, training_target.shape))\n",
    "print(\"validation input shape: %s, validation target shape: %s\"  % (validation_input.shape, validation_target.shape))\n",
    "print(\"test input shape: %s, test target shape: %s\"  % (test_input.shape, test_target.shape))\n",
    "# range of input values: 0 ... 255\n",
    "print(\"\\n\")\n",
    "\n",
    "# plot some sample images\n",
    "num_examples = ... # FIX!!!\n",
    "for s in range(num_examples):\n",
    "  print(\"Example image, true label: %d\" % training_target[s])\n",
    "  plt.imshow(training_input[s], vmin=0, vmax=255, cmap=plt.cm.gray)\n",
    "  plt.show()\n",
    "\n",
    "###-----------\n",
    "# process data\n",
    "###-----------\n",
    "\n",
    "# Note: shuffling is performed in fit method\n",
    "\n",
    "# scaling inputs from range 0 ... 255 to range [0,1] if desired\n",
    "scale_inputs = True # scale inputs to range [0,1]\n",
    "if scale_inputs:\n",
    "  training_input = training_input / 255\n",
    "  validation_input = validation_input / 255 \n",
    "  test_input = test_input / 255\n",
    "\n",
    "print(\"min. training data: %f\" % np.min(training_input))\n",
    "print(\"max. training data: %f\" % np.max(training_input))\n",
    "print(\"min. validation data: %f\" % np.min(validation_input))\n",
    "print(\"max. validation data: %f\" % np.max(validation_input))\n",
    "print(\"min. test data: %f\" % np.min(test_input))\n",
    "print(\"max. test data: %f\" % np.max(test_input))\n",
    "\n",
    "# histograms of input values\n",
    "nBins = 100\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15,10))\n",
    "axes[0].hist(training_input.flatten(), nBins)\n",
    "axes[0].set_xlabel(\"training\")\n",
    "axes[0].set_ylabel(\"counts\")\n",
    "axes[0].set_ylim((0,1e6))\n",
    "\n",
    "axes[1].hist(validation_input.flatten(), nBins)\n",
    "axes[1].set_xlabel(\"validation\")\n",
    "axes[1].set_ylabel(\"counts\")\n",
    "axes[1].set_ylim((0,1e6))\n",
    "axes[1].set_title('historgrams of input values')\n",
    "\n",
    "axes[2].hist(test_input.flatten(), nBins)\n",
    "axes[2].set_xlabel(\"test\")\n",
    "axes[2].set_ylabel(\"counts\")\n",
    "axes[2].set_ylim((0,1e6))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# flatten inputs to vectors\n",
    "training_input = training_input.reshape(training_input.shape[0], training_input.shape[1] * training_input.shape[2])\n",
    "validation_input = validation_input.reshape(validation_input.shape[0], validation_input.shape[1] * validation_input.shape[2])\n",
    "test_input = test_input.reshape(test_input.shape[0], test_input.shape[1] * test_input.shape[2])\n",
    "print(training_input.shape)\n",
    "print(validation_input.shape)\n",
    "print(test_input.shape)\n",
    "\n",
    "num_classes = ... # FIX!!!\n",
    "\n",
    "###-----------\n",
    "# define model\n",
    "###-----------\n",
    "\n",
    "histories = {}\n",
    "opt_learning_rate = {}\n",
    "final_training_loss = {}\n",
    "final_training_accuracy = {}\n",
    "final_validation_loss = {}\n",
    "final_validation_accuracy = {}\n",
    "final_test_loss = {}\n",
    "final_test_accuracy = {}\n",
    "\n",
    "configurations = [\n",
    "        # FIX!!!\n",
    "        # ... \n",
    "        {'learningRates': [...], # numpy array, e.g. [0.1, 0.2]\n",
    "         'hiddenLayerSizes': [...], # as before\n",
    "         'solver': '...',\n",
    "         'activation':'...'}, # activation of hidden layers\n",
    "         \n",
    "\n",
    "]\n",
    "\n",
    "learningRateSchedule = ... # FIX!!! True: apply (exponential) learning rate schedule; False: constant learning rate\n",
    "dropout = ... # FIX!!! 0 if no dropout, else fraction of dropout units (e.g. 0.2)   # FIX!!!\n",
    "batch_normalization = ... # FIX!!!\n",
    "regularization_weight = ... # FIX!!! 0 for no regularization or e.g. 0.01 to apply regularization\n",
    "regularizer = tfr.l1(l=regularization_weight) # or l2 or l1_l2; used for both weights and biases\n",
    "momentum = ... # FIX!!! 0 or e.g. 0.9, 0.99; ONLY FOR STOCHASTIC GRADIENT DESCENT AND RMSPROP\n",
    "nesterov = ... # FIX!!! ONLY FOR STOCHASTIC GRADIENT DESCENT\n",
    "\n",
    "numRepetitions = ... # FIX!!! repetitions of experiment due to stochastic nature\n",
    "\n",
    "num_inputs = training_input.shape[1] \n",
    "num_outputs = num_classes \n",
    "\n",
    "idx_config = 0\n",
    "\n",
    "for config in configurations:\n",
    "  print(\"=======\")\n",
    "  print(\"Now running tests for config\", config)\n",
    "\n",
    "  learningRates = config['learningRates']\n",
    "  num_hidden = config['hiddenLayerSizes']\n",
    "  solver = config['solver']\n",
    "  activation = config['activation']\n",
    "\n",
    "  # Sequential network structure.\n",
    "  model = Sequential()\n",
    "\n",
    "  if len(num_hidden) == 0:\n",
    "    print(\"Error: Must at least have one hidden layer!\")\n",
    "    sys.exit()  \n",
    "\n",
    "  # add first hidden layer connecting to input layer\n",
    "  model.add(Dense(num_hidden[0], input_dim=num_inputs, activation=activation, kernel_regularizer=regularizer, bias_regularizer=regularizer))\n",
    "\n",
    "#  if dropout: # dropout at input layer is generally not recommended\n",
    "#    # dropout of fraction dropout of the neurons and activation layer.\n",
    "#    model.add(Dropout(dropout))\n",
    "#  #  model.add(Activation(\"linear\"))\n",
    "\n",
    "  if batch_normalization:\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "  # potentially further hidden layers\n",
    "  for i in range(1, len(num_hidden)):\n",
    "    # add hidden layer with len[i] neurons\n",
    "    model.add(Dense(num_hidden[i], activation=activation, kernel_regularizer=regularizer, bias_regularizer=regularizer))\n",
    "  #  model.add(Activation(\"linear\"))\n",
    "\n",
    "    if dropout:\n",
    "    # dropout of fraction dropout of the neurons and activation layer.\n",
    "      model.add(Dropout(dropout))\n",
    "    #  model.add(Activation(\"linear\"))\n",
    "\n",
    "    if batch_normalization:\n",
    "      model.add(BatchNormalization())  \n",
    "\n",
    "  # output layer\n",
    "  model.add(Dense(units=num_outputs, name = \"output\", kernel_regularizer=regularizer, bias_regularizer=regularizer))\n",
    "\n",
    "  if dropout:\n",
    "  # dropout of fraction dropout of the neurons and activation layer.\n",
    "    model.add(Dropout(dropout))\n",
    "  #  model.add(Activation(\"linear\"))    \n",
    "    \n",
    "  # print configuration\n",
    "  print(\"\\nModel configuration: \")\n",
    "  print(model.get_config())\n",
    "  print(\"\\n\")\n",
    "\n",
    "  # show how the model looks\n",
    "  model.summary()\n",
    "\n",
    "  optLearningRate = 0\n",
    "  optValidationAccuracy = 0\n",
    "\n",
    "  histories_lr = [] # remember history for each learning rate\n",
    "\n",
    "  for idx_lr in range(len(learningRates)):\n",
    "  \n",
    "    print(\"MODIFYING LEARNING RATE\")\n",
    "    learningRate = learningRates[idx_lr]\n",
    "    if learningRateSchedule == True:\n",
    "      lr_schedule = schedules.ExponentialDecay(initial_learning_rate = learningRate, decay_steps=100000, decay_rate=0.96, staircase=True) # or PiecewiseConstantDecay or PolynomialDecay or InverseTimeDecay \n",
    "      print(\"... applying exponential decay learning rate schedule with initial learning rate %f\" % learningRate)\n",
    "    else:\n",
    "      lr_schedule = learningRate # constant learning rate\n",
    "      print(\"... constant learning rate %f\" % learningRate)\n",
    "\n",
    "    train_loss = np.zeros(numRepetitions)\n",
    "    train_acc = np.zeros(numRepetitions)\n",
    "    val_loss = np.zeros(numRepetitions)\n",
    "    val_acc = np.zeros(numRepetitions)\n",
    "    test_loss = np.zeros(numRepetitions)\n",
    "    test_acc = np.zeros(numRepetitions)\n",
    "\n",
    "    histories_rep = [] # (temporarily) remember history of each repetition\n",
    "    for idx_rep in range(numRepetitions):\n",
    "      print(\"\\nIteration %d...\" % idx_rep)  \n",
    "      \n",
    "      # compile model\n",
    "      if solver == 'SGD':        \n",
    "        opt = SGD(learning_rate=lr_schedule, momentum=momentum, nesterov=nesterov) # SGD or Adam, Nadam, Adadelta, Adagrad, RMSProp, potentially setting more parameters\n",
    "      elif solver == 'Adam':\n",
    "        opt = Adam(learning_rate=lr_schedule) \n",
    "      elif solver == 'Nadam':\n",
    "        opt = Nadam(learning_rate=lr_schedule) # Nadam doesn't support adaptive learning rate schedule!\n",
    "      elif solver == 'Adadelta':\n",
    "        opt = Adadelta(learning_rate=lr_schedule) \n",
    "      elif solver == 'Adagrad':\n",
    "        opt = Adagrad(learning_rate=lr_schedule) \n",
    "      elif solver == 'RMSprop':\n",
    "        opt = RMSprop(learning_rate=lr_schedule, momentum = momentum)\n",
    "      model.compile(optimizer=opt,loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),metrics=['sparse_categorical_accuracy'])\n",
    "\n",
    "      # Training model with train data. Fixed random seed:\n",
    "      num_epochs = ... # FIX !!!\n",
    "      batch_size = ... # FIX !!! \n",
    "      history = model.fit(training_input, training_target, epochs=num_epochs, batch_size=batch_size, shuffle=\"True\", verbose=2)\n",
    "      histories_rep.append(history) # remember all histories from all repetitions\n",
    "      train_loss[idx_rep] = history.history['loss'][num_epochs-1] \n",
    "      train_acc[idx_rep] = history.history['sparse_categorical_accuracy'][num_epochs-1]\n",
    "      val_loss[idx_rep] = model.evaluate(validation_input, validation_target)[0]\n",
    "      val_acc[idx_rep] = model.evaluate(validation_input, validation_target)[1]\n",
    "      test_loss[idx_rep] = model.evaluate(test_input, test_target)[0]\n",
    "      test_acc[idx_rep] = model.evaluate(test_input, test_target)[1]\n",
    "\n",
    "    # print results:\n",
    "    print(\"training loss (in brackets: mean +/- std):\")\n",
    "    for i in range(numRepetitions):\n",
    "        print(\"%f\" % train_loss[i])\n",
    "    print(\"(%f +/- %f)\\n\" % (np.mean(train_loss), np.std(train_loss, ddof=1)))\n",
    "\n",
    "    print(\"training accuracy (in brackets: mean +/- std):\")\n",
    "    for i in range(numRepetitions):\n",
    "        print(\"%f\" % train_acc[i])\n",
    "    print(\"(%f +/- %f)\\n\" % (np.mean(train_acc), np.std(train_acc, ddof=1)))\n",
    "\n",
    "    print(\"validation loss (in brackets: mean +/- std):\")\n",
    "    for i in range(numRepetitions):\n",
    "        print(\"%f\" % val_loss[i])\n",
    "    print(\"(%f +/- %f)\\n\" % (np.mean(val_loss), np.std(val_loss, ddof=1)))\n",
    "\n",
    "    print(\"validation accuracy (in brackets: mean +/- std):\")\n",
    "    for i in range(numRepetitions):\n",
    "        print(\"%f\" % val_acc[i])\n",
    "    print(\"(%f +/- %f)\\n\" % (np.mean(val_acc), np.std(val_acc, ddof=1)))\n",
    "\n",
    "    print(\"test loss (in brackets: mean +/- std):\")\n",
    "    for i in range(numRepetitions):\n",
    "        print(\"%f\" % test_loss[i])\n",
    "    print(\"(%f +/- %f)\\n\" % (np.mean(test_loss), np.std(test_loss, ddof=1)))\n",
    "\n",
    "    print(\"test accuracy (in brackets: mean +/- std):\")\n",
    "    for i in range(numRepetitions):\n",
    "        print(\"%f\" % test_acc[i])\n",
    "    print(\"(%f +/- %f)\\n\" % (np.mean(test_acc), np.std(test_acc, ddof=1)))\n",
    "\n",
    "    # remember history of best repetition (based on maximal validation accuracy)\n",
    "    idx_best_rep = np.argmax(val_acc)\n",
    "\n",
    "    # plot training loss and accuracy for best repetition\n",
    "    print(\"\\nbest repetition: experiment %d\" % idx_best_rep)\n",
    "    plt.plot(histories_rep[idx_best_rep].history['loss'], color = 'blue', \n",
    "                  label = 'training loss')\n",
    "    plt.plot(histories_rep[idx_best_rep].history['sparse_categorical_accuracy'], color = 'red', \n",
    "                  label = 'traning accuracy')\n",
    "    plt.xlabel('Epoch number')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # determine optimal learning rate (based on mean validation accuracy over repetitions)\n",
    "    if np.mean(val_acc) > optValidationAccuracy:\n",
    "        optValidationAccuracy = np.mean(val_acc)\n",
    "        opt_learning_rate[idx_config] = learningRate  \n",
    "        # remember history\n",
    "        histories[idx_config] = histories_rep[idx_best_rep]\n",
    "        # remember evaluation results\n",
    "        final_training_loss[idx_config] = train_loss[idx_best_rep]\n",
    "        final_training_accuracy[idx_config] = train_acc[idx_best_rep]\n",
    "        final_validation_loss[idx_config] = val_loss[idx_best_rep]\n",
    "        final_validation_accuracy[idx_config] = val_acc[idx_best_rep]\n",
    "        final_test_loss[idx_config] = test_loss[idx_best_rep]\n",
    "        final_test_accuracy[idx_config] = test_acc[idx_best_rep]   \n",
    "\n",
    "  print(\"\\n\\noptimal learning rate for this configuration: %f\\n\\n\" % opt_learning_rate[idx_config])\n",
    "\n",
    "  # print evaluation results\n",
    "  print(\"\\nconfiguration %s:\\n\" % configurations[idx_config])\n",
    "  print(\"optimal learning rate: %f\" % opt_learning_rate[idx_config])\n",
    "  print(\"final training loss: %f\" % final_training_loss[idx_config])\n",
    "  print(\"final training accuracy: %f\" % final_training_accuracy[idx_config])\n",
    "  print(\"final validation loss: %f\" % final_validation_loss[idx_config])\n",
    "  print(\"final validation accuracy: %f\" % final_validation_accuracy[idx_config])\n",
    "  print(\"final test loss: %f\" % final_test_loss[idx_config])\n",
    "  print(\"final test accuracy: %f\" % final_test_accuracy[idx_config])\n",
    "\n",
    "  # increment configuration index\n",
    "  idx_config = idx_config + 1\n",
    "\n",
    "###--------------------------------\n",
    "# Summary: print evaluation results\n",
    "###--------------------------------\n",
    "\n",
    "print(\"\\n\\nSummary:\\n\\n\")\n",
    "for i in range(len(configurations)): \n",
    "  print(\"\\nconfiguration %s:\\n\" % configurations[i])\n",
    "  print(\"optimal learning rate: %f\" % opt_learning_rate[i])\n",
    "  print(\"final training loss: %f\" % final_training_loss[i])\n",
    "  print(\"final training accuracy: %f\" % final_training_accuracy[i])\n",
    "  print(\"final validation loss: %f\" % final_validation_loss[i])\n",
    "  print(\"final validation accuracy: %f\" % final_validation_accuracy[i])\n",
    "  print(\"final test loss: %f\" % final_test_loss[i])\n",
    "  print(\"final test accuracy: %f\" % final_test_accuracy[i])\n",
    "\n",
    "###--------------------\n",
    "# Summary: plot results\n",
    "###--------------------\n",
    " \n",
    "# plot setup\n",
    "num_rows = np.int(np.ceil(len(configurations)/2))\n",
    "fig, axes = plt.subplots(num_rows, 2, figsize=(15, 10))\n",
    "fig.tight_layout() # improve spacing between subplots, doesn't work\n",
    "plt.subplots_adjust(left=0.125, right=0.9, bottom=0.1, top=0.9, wspace=0.2, hspace=0.2) # doesn't work\n",
    "legend = []\n",
    "i = 0\n",
    "axes_indices = {}\n",
    "\n",
    "if (len(configurations) <= 2):\n",
    "  for i in range(len(configurations)):\n",
    "    axes_indices[i] = i\n",
    "else:\n",
    "  for i in range(num_rows):\n",
    "    axes_indices[2*i] = (i, 0)\n",
    "    axes_indices[2*i+1] = (i, 1)\n",
    "\n",
    "for i in range(len(configurations)):\n",
    "  # plot loss    \n",
    "  axes[axes_indices[i]].set_title('configuration ' + str(i))\n",
    "  if i == 8 or i == 9:  \n",
    "    axes[axes_indices[i]].set_xlabel('Epoch number')\n",
    "  axes[axes_indices[i]].set_ylim(0, 1)\n",
    "  axes[axes_indices[i]].plot(histories[i].history['loss'], color = 'blue', \n",
    "              label = 'training loss')\n",
    "  axes[axes_indices[i]].plot(histories[i].history['sparse_categorical_accuracy'], color = 'red', \n",
    "              label = 'traning accuracy')\n",
    "  axes[axes_indices[i]].legend()\n",
    "\n",
    "  i = i + 1\n",
    "\n",
    "\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Answer: Write your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Exercise 4 (Vanishing gradient)\n",
    "\n",
    "a)\tThe Jupyter notebook implements a multi-layer perceptron for use on the MNIST digit classification problem. Apart from the training loss and accuracy, it also displays a histogram of the weights (between the input and the first hidden layer) after initialization and at the end of the training, and visualizes the weights (between the input layer and 16 hidden neurons of the first hidden layer). Using a sigmoid activation function, compare the output for a single hidden layer, five and six hidden layers. Then change to a ReLU activation function and inspect the results for six hidden layers. Discuss your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from os.path import join\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, BatchNormalization\n",
    "from tensorflow.keras import Model, Input, Sequential\n",
    "from tensorflow.keras.optimizers import SGD, Adam, Adadelta, Adagrad, Nadam, RMSprop, schedules\n",
    "from tensorflow.keras.utils import normalize\n",
    "import tensorflow.keras.datasets as tfds\n",
    "import tensorflow.keras.initializers as tfi\n",
    "import tensorflow.keras.regularizers as tfr\n",
    "\n",
    "###--------\n",
    "# load data\n",
    "###--------\n",
    "\n",
    "(training_input, training_target), (test_input, test_target)  = tfds.mnist.load_data()\n",
    "\n",
    "# Reserve 10,000 samples for validation\n",
    "validation_input = training_input[-10000:]\n",
    "validation_target = training_target[-10000:]\n",
    "training_input = training_input[:-10000]\n",
    "training_target = training_target[:-10000]\n",
    "\n",
    "print(\"training input shape: %s, training target shape: %s\"  % (training_input.shape, training_target.shape))\n",
    "print(\"validation input shape: %s, validation target shape: %s\"  % (validation_input.shape, validation_target.shape))\n",
    "print(\"test input shape: %s, test target shape: %s\"  % (test_input.shape, test_target.shape))\n",
    "# range of input values: 0 ... 255\n",
    "print(\"\\n\")\n",
    "\n",
    "###-----------\n",
    "# process data\n",
    "###-----------\n",
    "\n",
    "# Note: shuffling is performed in fit method\n",
    "\n",
    "# scaling inputs from range 0 ... 255 to range [0,1] if desired\n",
    "scale_inputs = True # scale inputs to range [0,1]\n",
    "if scale_inputs:\n",
    "  training_input = training_input / 255\n",
    "  validation_input = validation_input / 255 \n",
    "  test_input = test_input / 255\n",
    "\n",
    "# flatten inputs to vectors\n",
    "training_input = training_input.reshape(training_input.shape[0], training_input.shape[1] * training_input.shape[2])\n",
    "validation_input = validation_input.reshape(validation_input.shape[0], validation_input.shape[1] * validation_input.shape[2])\n",
    "test_input = test_input.reshape(test_input.shape[0], test_input.shape[1] * test_input.shape[2])\n",
    "print(training_input.shape)\n",
    "print(validation_input.shape)\n",
    "print(test_input.shape)\n",
    "\n",
    "num_classes = 10 # 10 digits\n",
    "\n",
    "###-----------\n",
    "# define model\n",
    "###-----------\n",
    "\n",
    "num_inputs = training_input.shape[1] \n",
    "num_hidden = [...] # FIX!!!\n",
    "num_outputs = num_classes \n",
    "\n",
    "initialLearningRate = 0.01 # FIX!!!\n",
    "# select constant learning rate or (flexible) learning rate schedule,\n",
    "# i.e. select one of the following two alternatives\n",
    "lr_schedule = initialLearningRate # constant learning rate\n",
    "# lr_schedule = schedules.ExponentialDecay(initial_learning_rate = initialLearningRate, decay_steps=100000, decay_rate=0.96, staircase=True) # or PiecewiseConstantDecay or PolynomialDecay or InverseTimeDecay \n",
    "\n",
    "solver = 'SGD'\n",
    "activation = '...' # FIX!!! e.g. sigmoid or relu\n",
    "dropout = 0 # 0 if no dropout, else fraction of dropout units (e.g. 0.2)   # FIX!!!\n",
    "batch_normalization = False\n",
    "\n",
    "weight_init = tfi.glorot_uniform() # FIX!!! default: glorot_uniform(); e.g. glorot_normal(), he_normal(), he_uniform(), lecun_normal(), lecun_uniform(), RandomNormal(), RandomUniform(), Zeros() etc.\n",
    "bias_init = tfi.Zeros() # FIX!!! default: Zeros(); for some possible values see weight initializers\n",
    "\n",
    "regularization_weight = 0.0 # 0 for no regularization or e.g. 0.01 to apply regularization\n",
    "regularizer = tfr.l1(l=regularization_weight) # or l2 or l1_l2; used for both weights and biases\n",
    "\n",
    "num_epochs = ... # FIX !!!\n",
    "batch_size = ... # FIX !!! \n",
    "\n",
    "# Sequential network structure.\n",
    "model = Sequential()\n",
    "\n",
    "if len(num_hidden) == 0:\n",
    "  print(\"Error: Must at least have one hidden layer!\")\n",
    "  sys.exit()  \n",
    "\n",
    "# add first hidden layer connecting to input layer\n",
    "\n",
    "model.add(Dense(num_hidden[0], input_dim=num_inputs, activation=activation, kernel_initializer=weight_init, bias_initializer = bias_init, kernel_regularizer=regularizer, bias_regularizer=regularizer))\n",
    "\n",
    "# if dropout: # dropout at input layer is generally not recommended\n",
    "#  # dropout of fraction dropout of the neurons and activation layer.\n",
    "#  model.add(Dropout(dropout))\n",
    "# #  model.add(Activation(\"linear\"))\n",
    "\n",
    "if batch_normalization:\n",
    "  model.add(BatchNormalization())\n",
    "\n",
    "# potentially further hidden layers\n",
    "for i in range(1, len(num_hidden)):\n",
    "  # add hidden layer with len[i] neurons\n",
    "  model.add(Dense(num_hidden[i], activation=activation, kernel_initializer=weight_init, bias_initializer = bias_init, kernel_regularizer=regularizer, bias_regularizer=regularizer))\n",
    "#  model.add(Activation(\"linear\"))\n",
    "  \n",
    "  if dropout:\n",
    "  # dropout of fraction dropout of the neurons and activation layer.\n",
    "    model.add(Dropout(dropout))\n",
    "  #  model.add(Activation(\"linear\"))\n",
    "\n",
    "  if batch_normalization:\n",
    "    model.add(BatchNormalization())  \n",
    "\n",
    "# output layer\n",
    "model.add(Dense(units=num_outputs, name = \"output\", kernel_initializer=weight_init, bias_initializer = bias_init, kernel_regularizer=regularizer, bias_regularizer=regularizer))\n",
    "\n",
    "if dropout:\n",
    "# dropout of fraction dropout of the neurons and activation layer.\n",
    "  model.add(Dropout(dropout))\n",
    "#  model.add(Activation(\"linear\"))\n",
    "  \n",
    "# print configuration\n",
    "print(\"\\nModel configuration: \")\n",
    "print(model.get_config())\n",
    "print(\"\\n\")\n",
    "print(\"... number of layers: %d\" % len(model.layers))\n",
    "\n",
    "# show how the model looks\n",
    "model.summary()\n",
    "      \n",
    "# compile model\n",
    "if solver == 'SGD':\n",
    "  momentum = 0 # e.g. 0.0, 0.5, 0.9 or 0.99\n",
    "  nesterov = False\n",
    "  opt = SGD(learning_rate=lr_schedule, momentum=momentum, nesterov=nesterov) # SGD or Adam, Nadam, Adadelta, Adagrad, RMSProp, potentially setting more parameters\n",
    "elif solver == 'Adam':\n",
    "  opt = Adam(learning_rate=lr_schedule)\n",
    "elif solver == 'Nadam':\n",
    "  opt = Adam(learning_rate=lr_schedule)\n",
    "elif solver == 'Adadelta':\n",
    "  opt = Adam(learning_rate=lr_schedule)\n",
    "elif solver == 'Adagrad':\n",
    "  opt = Adam(learning_rate=lr_schedule)\n",
    "elif solver == 'RMSprop':\n",
    "  opt = RMSprop(learning_rate=lr_schedule)\n",
    "model.compile(optimizer=opt,loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),metrics=['sparse_categorical_accuracy'])\n",
    "\n",
    "# histogram of weights (first layer) after initialization\n",
    "weights = model.layers[0].get_weights()[0]\n",
    "biases = model.layers[0].get_weights()[1]\n",
    "\n",
    "nBins = 100\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15,10))\n",
    "axes[0].hist(weights.flatten(), nBins)\n",
    "axes[0].set_xlabel(\"weights\")\n",
    "axes[0].set_ylabel(\"counts\")\n",
    "axes[0].set_title(\"weight histogram after initialization\")\n",
    "\n",
    "axes[1].hist(biases.flatten(), nBins)\n",
    "axes[1].set_xlabel(\"biases\")\n",
    "axes[1].set_ylabel(\"counts\")\n",
    "axes[1].set_title(\"bias histogram after initialization\")\n",
    "plt.show()\n",
    "\n",
    "# visualize the weights between input layer and some \n",
    "# of the hidden neurons of the first hidden layer after initialization\n",
    "# model.layers[0].get_weights()[0] is a (784 x numHiddenNeurons) array\n",
    "# model.layers[0].get_weights()[0].T (transpose) is a (numHiddenNeurons x 784) array,\n",
    "# the first entry of which contains the weights of all inputs connecting\n",
    "# to the first hidden neuron; those weights will be displayed in (28 x 28) format\n",
    "# until all plots (4 x 4, i.e. 16) are \"filled\" or no more hidden neurons are left\n",
    "print(\"Visualization of the weights between input and some of the hidden neurons of the first hidden layer:\")\n",
    "fig, axes = plt.subplots(4, 4, figsize=(15,15))\n",
    "# use global min / max to ensure all weights are shown on the same scale\n",
    "weights = model.layers[0].get_weights()[0]\n",
    "vmin, vmax = weights.min(), weights.max()\n",
    "for coef, ax in zip(weights.T, axes.ravel()):\n",
    "    ax.matshow(coef.reshape(28, 28), cmap=plt.cm.gray, vmin=.5 * vmin,\n",
    "               vmax=.5 * vmax)\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Training\n",
    "history = model.fit(training_input, training_target, epochs=num_epochs, batch_size=batch_size, shuffle=\"True\", verbose=2)\n",
    "\n",
    "# plot training loss and accuracy \n",
    "plt.plot(history.history['loss'], color = 'blue', label = 'training loss')\n",
    "plt.plot(history.history['sparse_categorical_accuracy'], color = 'red', label = 'traning accuracy')\n",
    "plt.xlabel('Epoch number')\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# model evaluation\n",
    "train_loss = history.history['loss'][num_epochs-1] \n",
    "train_acc = history.history['sparse_categorical_accuracy'][num_epochs-1]\n",
    "val_loss = model.evaluate(validation_input, validation_target)[0]\n",
    "val_acc = model.evaluate(validation_input, validation_target)[1]\n",
    "test_loss = model.evaluate(test_input, test_target)[0]\n",
    "test_acc = model.evaluate(test_input, test_target)[1]\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"final training loss: %f\" % train_loss)\n",
    "print(\"final training accuracy: %f\" % train_acc)\n",
    "print(\"final validation loss: %f\" % val_loss)\n",
    "print(\"final validation accuracy: %f\" % val_acc)\n",
    "print(\"final test loss: %f\" % test_loss)\n",
    "print(\"final test accuracy: %f\" % test_acc)\n",
    "print(\"\\n\")\n",
    "\n",
    "# histogram of weights (first layer) after training\n",
    "weights = model.layers[0].get_weights()[0]\n",
    "biases = model.layers[0].get_weights()[1]\n",
    "\n",
    "nBins = 100\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15,10))\n",
    "axes[0].hist(weights.flatten(), nBins)\n",
    "axes[0].set_xlabel(\"weights\")\n",
    "axes[0].set_ylabel(\"counts\")\n",
    "axes[0].set_title(\"weight histogram after training\")\n",
    "\n",
    "axes[1].hist(biases.flatten(), nBins)\n",
    "axes[1].set_xlabel(\"biases\")\n",
    "axes[1].set_ylabel(\"counts\")\n",
    "axes[1].set_title(\"bias histogram after training\")\n",
    "plt.show()\n",
    "\n",
    "# visualize the weights between input layer and some \n",
    "# of the hidden neurons of the first hidden layer after training\n",
    "# model.layers[0].get_weights()[0] is a (784 x numHiddenNeurons) array\n",
    "# model.layers[0].get_weights()[0].T (transpose) is a (numHiddenNeurons x 784) array,\n",
    "# the first entry of which contains the weights of all inputs connecting\n",
    "# to the first hidden neuron; those weights will be displayed in (28 x 28) format\n",
    "# until all plots (4 x 4, i.e. 16) are \"filled\" or no more hidden neurons are left\n",
    "print(\"Visualization of the weights between input and some of the hidden neurons of the first hidden layer:\")\n",
    "fig, axes = plt.subplots(4, 4, figsize=(15,15))\n",
    "# use global min / max to ensure all weights are shown on the same scale\n",
    "weights = model.layers[0].get_weights()[0]\n",
    "vmin, vmax = weights.min(), weights.max()\n",
    "for coef, ax in zip(weights.T, axes.ravel()):\n",
    "    ax.matshow(coef.reshape(28, 28), cmap=plt.cm.gray, vmin=.5 * vmin,\n",
    "               vmax=.5 * vmax)\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Answer: Write your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b)\tGive a theoretical justification, why the weights and biases of neurons in the first hidden layers in a multi-layer perceptron with many hidden layers are modified only slowly when using a sigmoid activation function and gradient descent. To this end, consider – as an example – a simplified network with three hidden layers (and a single neuron per layer), compute and analyse the change  of the bias of the first hidden neuron with respect to a change in the cost function C. What changes in your analysis when using a ReLU activation function instead of a sigmoid?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Answer: Write your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c)\tStarting from your analysis for the multi-layer perceptron with six hidden layers and sigmoid activation function in part a), try to find other model configurations which lead to a successful training. You may modify e.g. the learning rate and batch size, the weight and bias initialization, apply batch normalization and / or dropout, and add regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Answer: Write your answer here."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
