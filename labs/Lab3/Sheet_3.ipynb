{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MOitTdg3yP-M"
   },
   "source": [
    "# Important Information\n",
    "\n",
    "This file has two usages. \n",
    "1. This file can be displayed in Jupyter. You can read the task and insert your answers here. Start the notebook from an Anaconda prompt and change to the working directory containing the *.ipynb file.\n",
    "2. You can execute the code in Google Colab making use of Keras and Tensorflow. If you do not want to create a Google Account, you have to create a local environment for Keras and Tensorflow.\n",
    "\n",
    "For submission, convert your notebook with your solutions and all images to a pdf-file and upload this file into OLAT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-CXSNwExJb1C"
   },
   "source": [
    "Setup for this exercise sheet. Download data and define Tensorflow version.\n",
    "Execute code only if you setup your enviroment correctly or if you are inside a colab enviroment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 129
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Q7aTStJHJaay",
    "outputId": "7cd84744-a02f-4db8-ade2-21c81f6c4039",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! git clone https://gitlab+deploy-token-26:XBza882znMmexaQSpjad@git.informatik.uni-kiel.de/las/nndl.git\n",
    "\n",
    "%tensorflow_version 2.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1 (Learning in neural networks)\n",
    "\n",
    "a) Explain the following terms related to neural networks as short and precise as possible. \n",
    "\n",
    "* Learning in neural networks\n",
    "* Training set\n",
    "* Supervised Learning\n",
    "* Unsupervised Learning\n",
    "* Online (incremental) learning\n",
    "* Offline (batch) learning\n",
    "* Training error\n",
    "* Generalisation error\n",
    "* Overfitting\n",
    "* Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Answer\n",
    "\n",
    "Write your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d8ahoTHxBW82"
   },
   "source": [
    "b) Name and briefly describe at least two methods to indicate or avoid overfitting when training neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Answer\n",
    "\n",
    "Write your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dHDU9m2hChSn"
   },
   "source": [
    "# Exercise 2 (Perceptron learning – analytical calculation)\n",
    "\n",
    "The goal of this exercise is to train a single-layer perceptron (threshold element) to classify\n",
    "whether a fruit presented to the perceptron is going to be liked by a certain person or not,\n",
    "based on three features attributed to the presented fruit: its taste (whether it is sweet or not),\n",
    "its seeds (whether they are edible or not) and its skin (whether it is edible or not). This\n",
    "generates the following table for the inputs and the target output of the perceptron:\n",
    "\n",
    "Fruit | Input Taste<br>sweet = 1<br>not sweet = 0 | Input Seeds<br>edible = 1<br>not edible = 0 | Input Skin<br>edible = 1<br>not edible = 0 | Target output<br>person likes = 1<br>doesn’t like = 0\n",
    ":---|:---:|:---:|:---:|:---:\n",
    "Banana|1|1|0|1\n",
    "Pear|1|0|1|1\n",
    "Lemon|0|0|0|0\n",
    "Strawberry|1|1|1|1\n",
    "Green Apple|0|0|1|0\n",
    "\n",
    "Since there are three (binary) input values (taste, seeds and skin) and one (binary) target\n",
    "output, we will construct a single-layer perceptron with three inputs and one output.\n",
    "\n",
    "![IMAGE: perceptron](images/perceptron.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the target output is binary, we will use the perceptron learning algorithm to construct the weights.\n",
    "\n",
    "To start the perceptron learning algorithm, we have to initialize the weights and the threshold.\n",
    "Since we have no prior knowledge on the solution, we will assume that all weights are 0 ($w_1 = w_2 = w_3 = 0$) and that the threshold is $\\theta = 1$ (i.e. $w_0 = -\\theta = -1$).\n",
    "Furthermore, we have to specify the learning rate $\\eta$.\n",
    "Since we want it to be large enough that learning happens in a reasonable amount of time, but small enough so that it doesn’t go too fast, we set $\\eta = 0.25$.\n",
    "\n",
    "Apply the perceptron learning algorithm – in the incremental mode – analytically to this problem, i.e. calculate the new weights and threshold after successively presenting a banana, pear, lemon, strawberry and a green apple to the network (in this order).\n",
    "\n",
    "Draw a diagram of the final perceptron indicating the weight and threshold parameters and verify that the final perceptron classifies all training examples correctly.\n",
    "\n",
    "Note: The iteration of the perceptron learning algorithm is easily accomplished by filling in the following table for each iteration of the learning algorithm:\n",
    "\n",
    "First iteration ($\\mu = 1$), current training sample: banana\n",
    "\n",
    "Input<br> ${x}^{(\\mu)}$ | Current Weights<br>$w(t)$ | Network Output<br>$y^{(\\mu)}$ | Target Ouput<br>$d^{(\\mu)}$ | Learning rate<br>$\\eta$ | Weight Update<br>$\\Delta w(t)$ | New weights<br>$w(t+1)$\n",
    ":---:|:---:|:---:|:---:|:---:|:---:|:---:\n",
    "$x_0$ = 1 |  $w_0$ = | | | 0.25 | | \n",
    "$x_1$ =  |  $w_1$ = | | | 0.25 | | \n",
    "$x_2$ =  |  $w_2$ = | | | 0.25 | | \n",
    "$x_3$ =  |  $w_3$ = | | | 0.25 | | \n",
    "\n",
    "Second iteration ($\\mu = 2$), current training sample: pear\n",
    "...\n",
    "\n",
    "\n",
    "(Source of exercise: Langston, Cognitive Psychology)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Answer\n",
    "\n",
    "Write your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V0dZkCh_Cm2n"
   },
   "source": [
    "# Exercise 3 (Single-layer perceptron, gradient learning, 2dim. classification)\n",
    "\n",
    "The goal of this exercise is to solve a two-dimensional binary classification problem with gradient learning, using TensorFlow.\n",
    "Since the problem is two-dimensional, the perceptron has 2 inputs. Since the classification problem is binary, there is one output.\n",
    "\n",
    "The (two-dimensional) inputs for training are provided in the file *exercise3b_input.txt*, the corresponding (1-dimensional) targets in the file *exercise3b_target.txt*. To visualize the results, the training samples corresponding to class 1 (output label “0”) have separately been saved in the file *exercise3b_class1.txt*, the training samples corresponding to class 2 (output label “1”) in the file *exercise3b_class2.txt*.\n",
    "\n",
    "The gradient learning algorithm – using the sigmoid activation function – shall be used to provide a solution to this classification problem. Note that due to the sigmoid activation function, the output of the perceptron is a real value in [0,1]:\n",
    "\n",
    "\\begin{equation}\n",
    "sigmoid(h) = \\frac{1}{1+e^{-h}}\n",
    "\\end{equation}\n",
    "To assign a binary class label (either 0 or 1) to an input example, the perceptron output $y$ can\n",
    "be passed through the Heaviside function $\\theta [ y - 0.5 ] $ to yield a binary output $y^{binary}$.\n",
    "Then, any perceptron output between 0.5 and 1 is closer to 1 than to 0 and will be assigned the class label “1”.\n",
    "Conversely, any perceptron output between 0 and $<0.5$ is closer to 0 than to 1 and will be assigned the class label “0”.\n",
    "As usual, denote the weights of the perceptron $w_1$ and $w_2$ and the bias $w_0 = -\\theta $.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QUX94DzGW0Jh"
   },
   "source": [
    "## Task a)\n",
    "\n",
    "Using the above-mentioned post-processing step $\\theta [ y - 0.5 ] $  applied to the perceptron output $y$, show that the decision boundary separating the inputs $x=( x_1 , x_2 )$ assigned to class label “1” from those inputs assigned to class label “0” is given by a straight line in two-dimensional space corresponding to the equation (see in python code at *# plot last decision boundary*): \n",
    "\n",
    "$x_2 = -\\frac{w_1}{w_2}x_1 - \\frac{w_0}{w_2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Answer\n",
    "\n",
    "Write your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1bOBn82oXXU7"
   },
   "source": [
    "## Task b)\n",
    "\n",
    "The classification problem (defined by the training data provided in *exercise3b_input.txt* and the targets provided in *exercise3b_target.txt*)\n",
    "shall now be solved using the TensorFlow and Keras libraries.\n",
    "The source code is given below and can be executed by clicking the play button (in colab or in a local installation with tensorflow and keras).\n",
    "\n",
    "1.   Train the model at least three times (due to different random initializations) and report on your findings.\n",
    "2.   Change appropriate parameters (e.g. the learning rate, the batch size, the choice of the solver, potentially the number of epochs etc.) and again report on your findings.\n",
    "\n",
    "Useful information about training and evaluation with Tensorflow and Keras can be found at\n",
    "https://www.tensorflow.org/guide/keras/train_and_evaluate\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "B6tA_K3-BAyw",
    "outputId": "eff8b5af-1f29-46f9-b35b-ff6d276c63ca"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from os.path import join\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import Model, Input\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "\n",
    "###-----------------\n",
    "# load training data\n",
    "###-----------------\n",
    "path_to_task = \"nndl/Lab3\"\n",
    "input = np.loadtxt(join(path_to_task,'exercise3b_input.txt'))\n",
    "tmp = np.loadtxt(join(path_to_task,'exercise3b_target.txt'))\n",
    "target = np.array([tmp[i] for i in range(tmp.size)])\n",
    "class1 = np.loadtxt(join(path_to_task,'exercise3b_class1.txt'))\n",
    "class2 = np.loadtxt(join(path_to_task,'exercise3b_class2.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iF1QcuBwBFdW"
   },
   "source": [
    "\n",
    "Define the neural network, here you can change the structure of network, the learning rate and the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "HprUKTffLDnq"
   },
   "outputs": [],
   "source": [
    "# Define the structure\n",
    "input_layer = Input(shape=(2,), name='input') # two dimensional input\n",
    "out = Dense(units=1, activation=\"sigmoid\", name=\"output\")(input_layer) # one ouput node with sigmoid activation\n",
    "\n",
    "# create a model\n",
    "model = Model(input_layer, out)\n",
    "\n",
    "# show how the model looks\n",
    "model.summary()\n",
    "\n",
    "# compile the model\n",
    "opt = SGD(learning_rate=0.1)\n",
    "model.compile(optimizer=opt,loss=\"binary_crossentropy\",metrics=[\"acc\"])\n",
    "\n",
    "# try to invoke one of the weight initializers\n",
    "# initializer = tf.keras.initializers.GlorotUniform()\n",
    "# shape = (2,1) # n_in, n_out\n",
    "# random_weights = tf.Variable(initializer(shape=shape)) # returns tensor object; how to convert to numpy array needed for set_weights?\n",
    "# random_weights = np.random.uniform(low = -1.0, high = 1.0, size=(2,1))\n",
    "\n",
    "# weights: list; index 0: weights (numpy array of shape n_in x n_out), index 1: biases (numpy array)\n",
    "# model.set_weights([ random_weights, np.array([0])])\n",
    "\n",
    "# save initial weights\n",
    "initial_weights = model.layers[-1].get_weights() \n",
    "\n",
    "print(\"initial weights: (%f, %f)\" % (initial_weights[0][0], initial_weights[0][1]))\n",
    "print(\"initial bias: %f\" % initial_weights[1][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "idMWZMOMYMv5"
   },
   "source": [
    "This line actually trains the model. Changeable parameters batch_size and epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "q0YZ0ZFcOdI1"
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(x=input, y=target, batch_size=1, epochs=100, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sNL1qLxHYVbO"
   },
   "source": [
    "The following code snippet plots the results you create in the snippet before.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Dycq1s9UJ20r"
   },
   "outputs": [],
   "source": [
    "# plot setup\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 15))\n",
    "legend = []\n",
    "\n",
    "# plot the data\n",
    "axes[0].set_title('Toy classification problem: Data and decision boundaries')\n",
    "axes[0].set_xlabel('x1')\n",
    "axes[0].set_ylabel('x2')\n",
    "\n",
    "minx = min(input[:,0])\n",
    "maxx = max(input[:,0])\n",
    "miny = min(input[:,1])\n",
    "maxy = max(input[:,1])\n",
    "axes[0].set_xlim(minx, maxx)\n",
    "axes[0].set_ylim(miny, maxy) \n",
    "axes[0].plot(class1[:,0], class1[:,1], 'r.', \\\n",
    "    class2[:,0], class2[:,1], 'b.')\n",
    "legend.append('samples class1')\n",
    "legend.append('samples class2')\n",
    "\n",
    "# initial weights \n",
    "w0 = initial_weights[1][0] # bias\n",
    "# weight components (list of of numpy arrays of shape n_in x n_out)\n",
    "w1 = initial_weights[0][0][0]\n",
    "w2 = initial_weights[0][1][0]\n",
    "if ( w2 == 0 ):\n",
    "    print(\"Error: second weight zero!\")\n",
    "\n",
    "# calculate initial decision boundary\n",
    "interval = np.arange( np.floor(minx), np.ceil(maxx), 0.1 )\n",
    "initial_decision_boundary = -w1*interval/w2 - w0/w2\n",
    "\n",
    "# plot initial decision boundary\n",
    "args = {'c': 'black', 'linestyle': 'dashed'}\n",
    "axes[0].plot( interval, initial_decision_boundary, **args)\n",
    "legend.append('initial decision boundary')\n",
    "\n",
    "# get final weights\n",
    "final_weights = model.layers[-1].get_weights()\n",
    "w0 = final_weights[1][0] # bias\n",
    "# weight components (list of of numpy arrays of shape n_in x n_out)\n",
    "w1 = final_weights[0][0][0]\n",
    "w2 = final_weights[0][1][0]\n",
    "if ( w2 == 0 ):\n",
    "    print(\"Error: second weight zero!\")\n",
    "\n",
    "print(\"final weights: (%f, %f)\" % (final_weights[0][0], final_weights[0][1]))\n",
    "print(\"final bias: %f\" % final_weights[1][0])\n",
    "\n",
    "# calculate final decision boundary\n",
    "interval = np.arange( np.floor(minx), np.ceil(maxx), 0.1 )\n",
    "final_decision_boundary = -w1*interval/w2 - w0/w2\n",
    "\n",
    "# plot final decision boundary\n",
    "args = {'c': 'black', 'linestyle': '-'}\n",
    "axes[0].plot( interval, final_decision_boundary, **args)\n",
    "legend.append('final decision boundary')\n",
    "\n",
    "# plot training loss  \n",
    "axes[1].plot(history.history['loss'])\n",
    "axes[1].set_title('Toy classification problem: Loss curve')\n",
    "axes[1].set_xlabel('Epoch number')\n",
    "axes[1].set_ylim(0, 1)\n",
    "axes[1].set_ylabel('loss')\n",
    "\n",
    "# plot training accuracy  \n",
    "axes[2].plot(history.history['acc'])\n",
    "axes[2].set_title('Toy classification problem: acc curve')\n",
    "axes[2].set_ylim(0, 1)\n",
    "axes[2].set_xlabel('Epoch number')\n",
    "axes[2].set_ylabel('acc')\n",
    "\n",
    "# show the plot\n",
    "fig.legend(axes[0].get_lines(), legend, ncol=3, loc=\"upper center\")\n",
    "plt.show()\n",
    "\n",
    "# final evaluation (here: on the training data)\n",
    "eval = model.evaluate(x=input, y=target)\n",
    "print(\"Final loss: %f, final accuray: %f\" % (eval[0], eval[1]))\n",
    "\n",
    "predictions = model.predict(x=input)\n",
    "binary_predictions = np.heaviside(predictions - 0.5, 1) # second argument: output in case the input is 0\n",
    "binary_predictions = binary_predictions.reshape(target.shape)\n",
    "\n",
    "abs_binary_errors = np.where(binary_predictions != target)[0].size # np.where returns tuple, [0] recovers numpy array\n",
    "rel_binary_errors = abs_binary_errors / len(target)\n",
    "print(\"\\nnumber of binary errors: %d, error rate: %f, accuracy: %f\" % (abs_binary_errors, rel_binary_errors, 1.0 - rel_binary_errors ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Answer\n",
    "\n",
    "Write your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bCk4EokGYqSi"
   },
   "source": [
    "## Task c)\n",
    "\n",
    "Repeat exercise b) with the training set *exercise3c_input.txt* and the targets *exercise3c_target.txt*. Those points have been generated from the input points of exercise b) by removing points from class 1 (i.e. those points the x-coordinate of which is below 0.35). Do not forget to modify the variables *class1* and *class2* to load the files *exercise3c_class1.txt* and *exercise3c_class1.txt*, respectively! Discuss the output of the training algorithm in terms of the resulting decision boundary and the final training error.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Answer\n",
    "\n",
    "Write your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task d)\n",
    "\n",
    "Divide the input samples from part b) into separate training and validation sets, where the latter shall comprise 30% of the data. You may use available Keras functionality for this purpose. Run the script at least two times, plot the training and validation loss and accuracy as a function of the epoch number and report on your findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Answer\n",
    "\n",
    "Write your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task e)\n",
    "\n",
    "Modify the script to handle the XOR-problem, i.e. set\n",
    "\n",
    "input = np.array([[0,0],[0,1],[1,0],[1,1]])<br>\n",
    "target = np.array([0, 1, 1, 0])\n",
    "\n",
    "and plot the final decision boundary and the loss function. Report on your findings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Answer\n",
    "\n",
    "Write your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Exercise 4 (Multi-layer perceptron and backpropagation – small datasets)\n",
    "\n",
    "The goal of this exercise is to apply a multi-layer perceptron (MLP), trained with the backpropagation algorithm as provided by Tensorflow Keras library, to four classification problems provided by the UCI repository (and contained in the scikit learn package; i.e. iris, digits, wine, breast_cancer) and two artificially generated classification problems (circles, moon). In particular, the influence of the backpropagation solver and of the network topology shall be investigated in parts a) and b) of the exercise, respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task a)\n",
    "\n",
    "In this part of the exercise, a number of solvers (stochastic gradient descent, Adam, Adam with Nesterov momentum, AdaDelta, AdaGrad or RMSProp) shall be applied to the six datasets. An (incomplete) python script for this experiment is provided the Jupyter notebook. Complete the code (model definition, selection and configuration of an optimizer and model “compilation” including selection of an appropriate loss function; see *# TO BE ADAPTED* in the Jupyter notebook); consult the Tensorflow Keras documentation if needed. Furthermore, select suitable values of the most important parameters (e.g. learning rate, batch size...). Then, apply the script for at least three different optimizers, for a suitable baseline model configuration. Report the final training and validation loss and accuracy values and provide plots for the training and validation loss and accuracy curves as a function of the number of epochs (see script). What are your conclusions regarding the comparison of the optimization strategies? Also report on the database statistics.\n",
    "\n",
    "The optimizer is selected e.g. with <br>\n",
    "*opt = SGD(learning_rate=lr) # SGD or Adam, Nadam, Adadelta, Adagrad, RMSProp* <br>\n",
    "Note that additional parameters of the optimizers can be set if desired (see the Tensorflow Keras documentation).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from os.path import join\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import Model, Input\n",
    "from tensorflow.keras.optimizers import SGD, Adam, Adadelta, Adagrad, Nadam, RMSprop\n",
    "from tensorflow.keras.utils import normalize\n",
    "from sklearn import datasets\n",
    "\n",
    "###--------\n",
    "# load data\n",
    "###--------\n",
    "\n",
    "data_sets = ['iris', 'digits', 'wine', 'breast_cancer', 'circles', 'moons']\n",
    "histories = {}\n",
    "final_training_loss = {}\n",
    "final_training_accuracy = {}\n",
    "final_validation_loss = {}\n",
    "final_validation_accuracy = {}\n",
    "\n",
    "for name in data_sets:\n",
    "  print(\"\\nProcessing data set %s\" % name)\n",
    "  if name == 'iris':\n",
    "    iris = datasets.load_iris()\n",
    "    input = iris.data \n",
    "    target = iris.target \n",
    "  elif name == 'digits':    \n",
    "    digits = datasets.load_digits()\n",
    "    input = digits.data\n",
    "    target = digits.target\n",
    "  elif name == 'wine':\n",
    "    wine = datasets.load_wine() \n",
    "    input = wine.data\n",
    "    target = wine.target\n",
    "  elif name == 'breast_cancer':\n",
    "    breast_cancer = datasets.load_breast_cancer() \n",
    "    input = breast_cancer.data\n",
    "    target = breast_cancer.target\n",
    "  elif name == 'circles':\n",
    "    circles = datasets.make_circles(noise=0.2, factor=0.5, random_state=1)\n",
    "    input = circles[0]\n",
    "    target = circles[1]\n",
    "  elif name == 'moons':\n",
    "    moons = datasets.make_moons(noise=0.3, random_state=0)\n",
    "    input = moons[0]\n",
    "    target = moons[1]\n",
    "  else:\n",
    "    print(\"name %s unknown\" % name)\n",
    "  input_dim = input.shape[1]\n",
    "  print(\"input dimension: %d\" % input_dim)  \n",
    "  print(\"input shape: \" + str(input.shape))\n",
    "  print(\"target shape: \" + str(target.shape))\n",
    "  num_classes = len(np.unique(target))\n",
    "  print(\"number of classes: %d\" % num_classes)\n",
    "  print(\"class labels: \" + str(np.unique(target)))\n",
    "\n",
    "  ###-----------\n",
    "  # process data\n",
    "  ###-----------\n",
    "\n",
    "  # shuffle data\n",
    "  data = np.column_stack((input, target))\n",
    "  np.random.shuffle(data)\n",
    "  input = data[:,np.arange(input_dim)] # columns 0 ... input_dim - 1 (contain input features)\n",
    "  target = data[:,input_dim] # column input_dim (contains targets)\n",
    "\n",
    "  # normalize inputs\n",
    "  mean = np.mean(input)\n",
    "  std = np.std(input, ddof=1)\n",
    "  input = (input - mean) / std\n",
    "\n",
    "  # if necessary, transform labels to be in range 0 ... num_classes - 1\n",
    "  labels_for_one_hot = {}\n",
    "  for i in range(num_classes):\n",
    "    labels_for_one_hot[np.unique(target)[i]] = i\n",
    "\n",
    "  # one-hot encoding\n",
    "  def one_hot(j):\n",
    "    vec = np.zeros(num_classes)\n",
    "    vec[j] = 1\n",
    "    return vec\n",
    "\n",
    "  # transform targets to one-hot encoding\n",
    "  target_one_hot = np.zeros((len(target), num_classes))\n",
    "  for i in range(len(target)):\n",
    "    target_one_hot[i] = one_hot( labels_for_one_hot[int(target[i])] )\n",
    "\n",
    "  ###-----------\n",
    "  # define model\n",
    "  ###-----------\n",
    "\n",
    "  # Define the structure of the neural network\n",
    "  num_inputs = input_dim\n",
    "  num_hidden = 100 # TO BE ADAPTED\n",
    "  num_outputs = num_classes\n",
    "  input_layer = Input(shape=(num_inputs,), name='input') # two dimensional input\n",
    "  hidden_1 = ... # TO BE ADAPTED\n",
    "  out = ... # TO BE ADAPTED \n",
    "  \n",
    "  # create a model\n",
    "  model = Model(input_layer, out)\n",
    "\n",
    "  # show how the model looks\n",
    "  model.summary()\n",
    "\n",
    "  # compile the model\n",
    "  lr = 0.01 # TO BE ADAPTED\n",
    "  opt = ... # TO BE ADAPTED\n",
    "  model.compile(optimizer=opt,loss=\"...\",metrics=[\"...\"]) # TO BE ADAPTED\n",
    "\n",
    "  ###-------\n",
    "  # training\n",
    "  ###-------\n",
    "\n",
    "  # Train the model\n",
    "  num_epochs = 100 # TO BE ADAPTED\n",
    "  batch_size = 1 # TO BE ADAPTED\n",
    "  history = model.fit(x=input, y=target_one_hot, batch_size=batch_size, epochs=num_epochs, verbose=True, validation_split = 0.3)\n",
    "  histories[name] = history\n",
    "  final_training_loss[name] = history.history['loss'][num_epochs-1]\n",
    "  final_training_accuracy[name] = history.history['categorical_accuracy'][num_epochs-1]\n",
    "  final_validation_loss[name] = history.history['val_loss'][num_epochs-1]\n",
    "  final_validation_accuracy[name] = history.history['val_categorical_accuracy'][num_epochs-1]\n",
    "\n",
    "for name in data_sets: \n",
    "  print(\"\\n%s:\\n\" % name)\n",
    "  print(\"final training loss: %f\" % final_training_loss[name])\n",
    "  print(\"final training accuracy: %f\" % final_training_accuracy[name])\n",
    "  print(\"final validation loss: %f\" % final_validation_loss[name])\n",
    "  print(\"final validation accuracy: %f\" % final_validation_accuracy[name])\n",
    "\n",
    "###-----------\n",
    "# plot results\n",
    "###-----------\n",
    " \n",
    "# plot setup\n",
    "fig, axes = plt.subplots(3, 2, figsize=(15, 10))\n",
    "fig.tight_layout() # improve spacing between subplots, doesn't work\n",
    "plt.subplots_adjust(left=0.125, right=0.9, bottom=0.1, top=0.9, wspace=0.2, hspace=0.2) # doesn't work\n",
    "legend = []\n",
    "i = 0\n",
    "axes_indices = {0 : (0,0), 1 : (0,1), 2: (1,0), 3: (1,1), 4: (2,0), 5: (2,1)}\n",
    "\n",
    "for name in data_sets:\n",
    "  # plot loss    \n",
    "  axes[axes_indices[i]].set_title(name)\n",
    "  if i == 4 or i == 5:  \n",
    "    axes[axes_indices[i]].set_xlabel('Epoch number')\n",
    "  axes[axes_indices[i]].set_ylim(0, 1)\n",
    "  axes[axes_indices[i]].plot(histories[name].history['loss'], color = 'blue', \n",
    "              label = 'training loss')\n",
    "  axes[axes_indices[i]].plot(histories[name].history['val_loss'], color = 'red', \n",
    "              label = 'validation loss')\n",
    "  axes[axes_indices[i]].legend()\n",
    "\n",
    "  # plot accuracy  \n",
    "  axes[axes_indices[i]].plot(histories[name].history['categorical_accuracy'], color = 'blue', linestyle = 'dashed',\n",
    "              label = 'training accuracy')\n",
    "  axes[axes_indices[i]].plot(histories[name].history['val_categorical_accuracy'], color = 'red', linestyle = 'dashed', \n",
    "              label = 'validation accuracy')\n",
    "  axes[axes_indices[i]].legend()\n",
    "  i = i + 1\n",
    "\n",
    "\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Answer\n",
    "\n",
    "Write your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task b)\n",
    "\n",
    "Using the most successful optimizer from part a), in this part of the exercise different network topologies shall be investigated, i.e. the number of hidden layers and of hidden neurons shall be varied. To this end, modify the python script accordingly and systematically test the network performance. Provide the final training and validation loss and accuracy and provide the loss and accuracy curves as function of the number of epochs. You may also test further parameter settings. What are your conclusions regarding the network topology?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Answer\n",
    "\n",
    "Write your answer here."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
